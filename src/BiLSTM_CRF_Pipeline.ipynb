{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "3f78b250",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import Dataset\n",
                "from torch.utils.data import DataLoader\n",
                "from tqdm import tqdm\n",
                "import os\n",
                "import csv\n",
                "import numpy as np\n",
                "import re\n",
                "from typing import List, Any\n",
                "\n",
                "# PyTorch CRF\n",
                "from torch.cuda.amp import autocast, GradScaler\n",
                "from torchcrf import CRF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "1cddf29e",
            "metadata": {},
            "outputs": [],
            "source": [
                "model_path = f\"../models/ArabicBiLSTMCRFModel.pth\"\n",
                "input_path = f\"../input/test_no_diacritics.txt\"\n",
                "output_path = f\"../output/output_bilstm_crf_main.txt\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9c26ec0b",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Configurations\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "# Global registries\n",
                "DATASET_REGISTRY: dict[str, Any] = {}\n",
                "MODEL_REGISTRY: dict[str, Any] = {}\n",
                "\n",
                "# Model hyperparameters\n",
                "EMBEDDING_DIM = 128\n",
                "HIDDEN_DIM = 256\n",
                "BATCH_SIZE = 128\n",
                "NUM_EPOCHS = 6\n",
                "LEARNING_RATE = 0.001\n",
                "NUM_LAYERS = 3\n",
                "DROPOUT = 0.2\n",
                "\n",
                "# Data parameters\n",
                "ARABIC_LETTERS = sorted(\n",
                "    np.load('../data/utils/arabic_letters.pkl', allow_pickle=True))\n",
                "DIACRITICS = sorted(np.load(\n",
                "    '../data/utils/diacritics.pkl', allow_pickle=True))\n",
                "PUNCTUATIONS = {\".\", \"،\", \":\", \"؛\", \"؟\", \"!\", '\"', \"-\"}\n",
                "\n",
                "VALID_CHARS = set(ARABIC_LETTERS).union(\n",
                "    set(DIACRITICS)).union(PUNCTUATIONS).union({\" \"})\n",
                "\n",
                "CHAR2ID = {char: id for id, char in enumerate(ARABIC_LETTERS)}\n",
                "CHAR2ID[\" \"] = len(ARABIC_LETTERS)\n",
                "CHAR2ID[\"<PAD>\"] = len(ARABIC_LETTERS) + 1\n",
                "PAD = CHAR2ID[\"<PAD>\"]\n",
                "SPACE = CHAR2ID[\" \"]\n",
                "ID2CHAR = {id: char for char, id in CHAR2ID.items()}\n",
                "\n",
                "DIACRITIC2ID = np.load('../data/utils/diacritic2id.pkl', allow_pickle=True)\n",
                "ID2DIACRITIC = {id: diacritic for diacritic, id in DIACRITIC2ID.items()}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "7aa107dd",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def register_dataset(name):\n",
                "    def decorator(cls):\n",
                "        DATASET_REGISTRY[name] = cls\n",
                "        return cls\n",
                "    return decorator\n",
                "\n",
                "\n",
                "def generate_dataset(dataset_name: str, *args, **kwargs):\n",
                "    try:\n",
                "        dataset_cls = DATASET_REGISTRY[dataset_name]\n",
                "    except KeyError:\n",
                "        raise ValueError(f\"Dataset '{dataset_name}' is not recognized.\")\n",
                "    return dataset_cls(*args, **kwargs)\n",
                "\n",
                "\n",
                "\n",
                "def register_model(name):\n",
                "    def decorator(cls):\n",
                "        MODEL_REGISTRY[name] = cls\n",
                "        return cls\n",
                "    return decorator\n",
                "\n",
                "\n",
                "def generate_model(model_name: str, *args, **kwargs):\n",
                "    try:\n",
                "        model_cls = MODEL_REGISTRY[model_name]\n",
                "    except KeyError:\n",
                "        raise ValueError(f\"Model '{model_name}' is not recognized.\")\n",
                "    return model_cls(*args, **kwargs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "85a77ff8",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "@register_dataset(\"ArabicDataset\")\n",
                "class ArabicDataset(Dataset):\n",
                "    def __init__(self, file_path: str):\n",
                "        self.data_X, self.data_Y = self.generate_tensor_data(file_path)\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.data_X)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        return self.data_X[idx], self.data_Y[idx]\n",
                "\n",
                "    def generate_tensor_data(self, data_path: str):\n",
                "        data_Y = self.load_data(data_path)\n",
                "        data_X = self.extract_text_without_diacritics(data_Y)\n",
                "\n",
                "        encoded_data_X, encoded_data_Y = self.encode_data(data_X, data_Y)\n",
                "        data_X = torch.tensor(\n",
                "            encoded_data_X, dtype=torch.int64)\n",
                "        data_Y = torch.tensor(\n",
                "            encoded_data_Y, dtype=torch.int64)\n",
                "\n",
                "        return data_X, data_Y\n",
                "\n",
                "    def load_data(self, file_path: str):\n",
                "        data = []\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            for line in f:\n",
                "                line = line.strip()\n",
                "                if line:\n",
                "                    line = re.sub(\n",
                "                        f'[^{re.escape(\"\".join(VALID_CHARS))}]', '', line)\n",
                "                    line = re.sub(r'\\s+', ' ', line)\n",
                "                    sentences = re.split(\n",
                "                        f'[{re.escape(\"\".join(PUNCTUATIONS))}]', line)\n",
                "                    sentences = [s.strip() for s in sentences if s.strip()]\n",
                "                    data.extend(sentences)\n",
                "\n",
                "        return np.array(data)\n",
                "\n",
                "    def extract_text_without_diacritics(self, dataY):\n",
                "        dataX = dataY.copy()\n",
                "        for diacritic, _ in DIACRITIC2ID.items():\n",
                "            dataX = np.char.replace(\n",
                "                dataX, diacritic, '')\n",
                "        return dataX\n",
                "\n",
                "    def encode_data(self, dataX: List[str], dataY: List[str]):\n",
                "        encoded_data_X = []\n",
                "        for sentence in dataX:\n",
                "            encoded_data_X.append([CHAR2ID[char]\n",
                "                                   for char in sentence if char in CHAR2ID])\n",
                "        encoded_data_Y = []\n",
                "        for sentence in dataY:\n",
                "            encoded_data_Y.append(self.extract_diacritics(sentence))\n",
                "\n",
                "        max_sentence_len = max(len(sentence) for sentence in encoded_data_X)\n",
                "        padded_dataX = np.full(\n",
                "            (len(encoded_data_X), max_sentence_len), PAD, dtype=np.int64)\n",
                "        for i, seq in enumerate(encoded_data_X):\n",
                "            padded_dataX[i, :len(seq)] = seq\n",
                "\n",
                "        padded_dataY = np.full(\n",
                "            (len(encoded_data_Y), max_sentence_len), 0, dtype=np.int64)  # Use 0 instead of PAD for CRF\n",
                "        for i, seq in enumerate(encoded_data_Y):\n",
                "            padded_dataY[i, :len(seq)] = seq\n",
                "\n",
                "        return padded_dataX, padded_dataY\n",
                "\n",
                "    def extract_diacritics(self, sentence: str):\n",
                "        result = []\n",
                "        i = 0\n",
                "        n = len(sentence)\n",
                "        on_char = False\n",
                "\n",
                "        while i < n:\n",
                "            ch = sentence[i]\n",
                "            if ch in DIACRITICS:\n",
                "                on_char = False\n",
                "                # check if next char forms a stacked diacritic\n",
                "                if i+1 < n and sentence[i+1] in DIACRITICS:\n",
                "                    combined = ch + sentence[i+1]\n",
                "                    if combined in DIACRITIC2ID:\n",
                "                        result.append(DIACRITIC2ID[combined])\n",
                "                        i += 2\n",
                "                        continue\n",
                "                result.append(DIACRITIC2ID[ch])\n",
                "            elif ch in CHAR2ID:\n",
                "                if on_char:\n",
                "                    result.append(DIACRITIC2ID[''])\n",
                "                on_char = True\n",
                "            i += 1\n",
                "        if on_char:\n",
                "            result.append(DIACRITIC2ID[''])\n",
                "        return result"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "6270c87e",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "@register_model(\"BiLSTMCRFArabicModel\")\n",
                "class BiLSTMCRFArabicModel(nn.Module):\n",
                "    \"\"\"\n",
                "    Bi-LSTM + CRF Model for Arabic Diacritization.\n",
                "    Same as LSTMArabicModel but with CRF layer for sequence-level constraints.\n",
                "    \"\"\"\n",
                "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, PAD):\n",
                "        super(BiLSTMCRFArabicModel, self).__init__()\n",
                "        self.embedding_dim = embedding_dim\n",
                "        self.hidden_dim = hidden_dim\n",
                "        self.output_dim = output_dim\n",
                "        self.PAD = PAD\n",
                "        \n",
                "        # Character embedding layer\n",
                "        self.embedding = nn.Embedding(\n",
                "            vocab_size, embedding_dim, padding_idx=PAD)\n",
                "        \n",
                "        # Bi-LSTM layer\n",
                "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
                "                            batch_first=True, bidirectional=True,\n",
                "                            num_layers=NUM_LAYERS, dropout=DROPOUT)\n",
                "        \n",
                "        # Linear layer to project LSTM output to tag space\n",
                "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
                "        \n",
                "        # CRF layer for sequence-level decoding\n",
                "        self.crf = CRF(output_dim, batch_first=True)\n",
                "\n",
                "    def _get_lstm_features(self, x):\n",
                "        \"\"\"\n",
                "        Get emission scores from Bi-LSTM.\n",
                "        Args:\n",
                "            x: Input tensor (batch_size, seq_len)\n",
                "        Returns:\n",
                "            emissions: Tensor (batch_size, seq_len, output_dim)\n",
                "        \"\"\"\n",
                "        embedded = self.embedding(x)\n",
                "        lstm_out, _ = self.lstm(embedded)\n",
                "        emissions = self.fc(lstm_out)\n",
                "        return emissions\n",
                "\n",
                "    def forward(self, x, tags=None, mask=None):\n",
                "        \"\"\"\n",
                "        Forward pass.\n",
                "        During training (tags provided): returns CRF loss\n",
                "        During inference (tags=None): returns best tag sequence\n",
                "        \"\"\"\n",
                "        emissions = self._get_lstm_features(x)\n",
                "        \n",
                "        if tags is not None:\n",
                "            # Training mode: return negative log-likelihood loss\n",
                "            if mask is None:\n",
                "                mask = (x != self.PAD)\n",
                "            loss = -self.crf(emissions, tags, mask=mask, reduction='mean')\n",
                "            return loss\n",
                "        else:\n",
                "            # Inference mode: decode best sequence\n",
                "            if mask is None:\n",
                "                mask = (x != self.PAD)\n",
                "            return self.crf.decode(emissions, mask=mask)\n",
                "\n",
                "    def decode(self, x, mask=None):\n",
                "        \"\"\"Decode the best tag sequence using Viterbi algorithm.\"\"\"\n",
                "        emissions = self._get_lstm_features(x)\n",
                "        if mask is None:\n",
                "            mask = (x != self.PAD)\n",
                "        return self.crf.decode(emissions, mask=mask)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "a782bf2f",
            "metadata": {},
            "outputs": [],
            "source": [
                "def train(model: nn.Module, train_dataset: Dataset, val_dataset: Dataset, model_path: str, patience: int = 3):\n",
                "    \"\"\"\n",
                "    Train the model with validation monitoring and early stopping.\n",
                "    \n",
                "    Args:\n",
                "        model: The BiLSTM-CRF model\n",
                "        train_dataset: Training dataset\n",
                "        val_dataset: Validation dataset\n",
                "        model_path: Path to save the best model\n",
                "        patience: Number of epochs to wait for improvement before stopping (0 = disabled)\n",
                "    \"\"\"\n",
                "    train_data_loader = DataLoader(\n",
                "        train_dataset,\n",
                "        batch_size=BATCH_SIZE,\n",
                "        shuffle=True,\n",
                "        num_workers=2,\n",
                "        pin_memory=True,\n",
                "        persistent_workers=True\n",
                "    )\n",
                "    \n",
                "    val_data_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
                "    \n",
                "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
                "    scaler = GradScaler()  # For mixed precision training\n",
                "\n",
                "    if torch.cuda.is_available():\n",
                "        model = model.cuda()\n",
                "\n",
                "    best_val_acc = 0.0\n",
                "    epochs_without_improvement = 0\n",
                "    \n",
                "    for epoch in range(NUM_EPOCHS):\n",
                "        # Training phase\n",
                "        total_loss = 0\n",
                "        num_batches = 0\n",
                "\n",
                "        model.train()\n",
                "        for train_X, train_Y in tqdm(train_data_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
                "            train_X = train_X.to(DEVICE)\n",
                "            train_Y = train_Y.to(DEVICE)\n",
                "            mask = (train_X != PAD)\n",
                "\n",
                "            optimizer.zero_grad()\n",
                "\n",
                "            # Mixed precision forward pass\n",
                "            with autocast():\n",
                "                loss = model(train_X, tags=train_Y, mask=mask)\n",
                "\n",
                "            # Mixed precision backward pass\n",
                "            scaler.scale(loss).backward()\n",
                "            scaler.unscale_(optimizer)\n",
                "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
                "            scaler.step(optimizer)\n",
                "            scaler.update()\n",
                "\n",
                "            total_loss += loss.item()\n",
                "            num_batches += 1\n",
                "\n",
                "        avg_loss = total_loss / num_batches\n",
                "        print(f'Epoch {epoch + 1} | Train Loss: {avg_loss:.4f}')\n",
                "        \n",
                "        # Validation phase\n",
                "        model.eval()\n",
                "        total_correct = 0\n",
                "        total_tokens = 0\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            for val_X, val_Y in tqdm(val_data_loader, desc=f\"Validation Epoch {epoch + 1}\"):\n",
                "                val_X = val_X.to(DEVICE)\n",
                "                val_Y = val_Y.to(DEVICE)\n",
                "                mask = (val_X != PAD)\n",
                "\n",
                "                # Decode using CRF\n",
                "                predictions_list = model.decode(val_X, mask=mask)\n",
                "                \n",
                "                # Convert to tensor\n",
                "                batch_size, seq_len = val_X.shape\n",
                "                prediction = torch.full_like(val_Y, 0)\n",
                "                for i, pred_seq in enumerate(predictions_list):\n",
                "                    prediction[i, :len(pred_seq)] = torch.tensor(pred_seq, device=DEVICE)\n",
                "\n",
                "                padding_mask = (val_X == PAD)\n",
                "                everything_mask = ~padding_mask\n",
                "\n",
                "                total_correct += ((prediction == val_Y) & everything_mask).sum().item()\n",
                "                total_tokens += everything_mask.sum().item()\n",
                "\n",
                "        val_accuracy = (total_correct / total_tokens) * 100\n",
                "        print(f'Epoch {epoch + 1} | Validation Accuracy: {val_accuracy:.2f}%')\n",
                "        \n",
                "        # Save best model\n",
                "        if val_accuracy > best_val_acc:\n",
                "            best_val_acc = val_accuracy\n",
                "            epochs_without_improvement = 0\n",
                "            torch.save(model.state_dict(), model_path)\n",
                "            print(f'  → New best model saved! (Accuracy: {val_accuracy:.2f}%)')\n",
                "        else:\n",
                "            epochs_without_improvement += 1\n",
                "            print(f'  → No improvement ({epochs_without_improvement}/{patience})')\n",
                "        \n",
                "        # Early stopping\n",
                "        if patience > 0 and epochs_without_improvement >= patience:\n",
                "            print(f'\\nEarly stopping triggered after {epoch + 1} epochs')\n",
                "            print(f'Best validation accuracy: {best_val_acc:.2f}%')\n",
                "            break\n",
                "    \n",
                "    print(f'\\nTraining completed!')\n",
                "    print(f'Best validation accuracy: {best_val_acc:.2f}%')\n",
                "    print(f'Model saved to {model_path}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "f00cc728",
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate(model: torch.nn.Module, val_dataset: torch.utils.data.Dataset):\n",
                "\n",
                "    val_data_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
                "\n",
                "    if torch.cuda.is_available():\n",
                "        model = model.cuda()\n",
                "\n",
                "    total_correct_without_ending = 0\n",
                "    total_tokens_without_ending = 0\n",
                "    total_correct_ending = 0\n",
                "    total_tokens_ending = 0\n",
                "    total_correct = 0\n",
                "    total_tokens = 0\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "\n",
                "        for val_X, val_Y in tqdm(val_data_loader):\n",
                "            val_X = val_X.to(DEVICE)\n",
                "            val_Y = val_Y.to(DEVICE)\n",
                "            mask = (val_X != PAD)\n",
                "\n",
                "            # Decode using CRF\n",
                "            predictions_list = model.decode(val_X, mask=mask)\n",
                "            \n",
                "            # Convert list of predictions to tensor for comparison\n",
                "            batch_size, seq_len = val_X.shape\n",
                "            prediction = torch.full_like(val_Y, 0)\n",
                "            for i, pred_seq in enumerate(predictions_list):\n",
                "                prediction[i, :len(pred_seq)] = torch.tensor(pred_seq, device=DEVICE)\n",
                "\n",
                "            padding_mask = (val_X == PAD)  # Identify padding from inputs, not labels\n",
                "            shifted = torch.roll(val_X, shifts=-1, dims=1)\n",
                "            end_of_word_mask = (shifted == SPACE) | (shifted == PAD)\n",
                "\n",
                "            last_char_mask = end_of_word_mask & (~padding_mask)\n",
                "            rest_of_word_mask = (~end_of_word_mask) & (~padding_mask)\n",
                "            everything_mask = ~padding_mask\n",
                "\n",
                "            total_correct_ending += ((prediction == val_Y)\n",
                "                                     & last_char_mask).sum().item()\n",
                "            total_tokens_ending += last_char_mask.sum().item()\n",
                "\n",
                "            total_correct_without_ending += ((prediction == val_Y) &\n",
                "                                             rest_of_word_mask).sum().item()\n",
                "            total_tokens_without_ending += rest_of_word_mask.sum().item()\n",
                "\n",
                "            total_correct += ((prediction == val_Y) &\n",
                "                              everything_mask).sum().item()\n",
                "            total_tokens += everything_mask.sum().item()\n",
                "\n",
                "        val_accuracy = (total_correct / total_tokens) * 100\n",
                "        val_accuracy_without_ending = (total_correct_without_ending /\n",
                "                                       total_tokens_without_ending) * 100\n",
                "        val_accuracy_ending = (total_correct_ending /\n",
                "                               total_tokens_ending) * 100\n",
                "        print(\n",
                "            f\"Validation Accuracy (Overall): {val_accuracy:.2f}%\\n\" +\n",
                "            f\"Validation Accuracy (Without Last Character): {val_accuracy_without_ending:.2f}%\\n\" +\n",
                "            f\"Validation Accuracy (Last Character): {val_accuracy_ending:.2f}%\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "3ed91e8d",
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict(model, encoded_sentence):\n",
                "    input_tensor = torch.tensor(\n",
                "        [encoded_sentence], dtype=torch.int64).to(DEVICE)\n",
                "    mask = (input_tensor != PAD)\n",
                "    with torch.no_grad():\n",
                "        predictions_list = model.decode(input_tensor, mask=mask)\n",
                "    return np.array(predictions_list[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "02a98acc",
            "metadata": {},
            "outputs": [],
            "source": [
                "def infer(model, model_path, input_path, output_path, text_path=None):\n",
                "    \"\"\"\n",
                "    Run inference on input data and generate diacritized output.\n",
                "\n",
                "    Args:\n",
                "        model: The model to use for inference\n",
                "        model_path: Path to model weights\n",
                "        input_path: Path to input file (CSV or TXT)\n",
                "        output_path: Path to output file\n",
                "        text_path: Path to text file (required for CSV input to get full context)\n",
                "    \"\"\"\n",
                "    model_state_dict = torch.load(model_path, map_location=DEVICE)\n",
                "    model.load_state_dict(model_state_dict)\n",
                "\n",
                "    # Check if input is CSV format (submission format) or text format\n",
                "    is_csv_input = input_path.endswith(\".csv\")\n",
                "\n",
                "    if is_csv_input:\n",
                "        # Read CSV with id,line_number,letter format (may have case_ending column)\n",
                "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
                "            reader = csv.DictReader(f)\n",
                "            rows = list(reader)\n",
                "\n",
                "        # Build a set of IDs we need to output predictions for\n",
                "        target_ids = {int(row[\"id\"]) for row in rows}\n",
                "\n",
                "        # Read the text file to get full sentences for context\n",
                "        if text_path is None:\n",
                "            text_path = os.path.join(\n",
                "                os.path.dirname(input_path), \"dataset_no_diacritics.txt\"\n",
                "            )\n",
                "\n",
                "        with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
                "            input_lines = f.readlines()\n",
                "\n",
                "        # Store predictions with their IDs\n",
                "        output_csv = [[\"ID\", \"label\"]]\n",
                "        output_list = []\n",
                "        current_id = 0  # Global character ID counter\n",
                "\n",
                "        model.eval()\n",
                "        for sentence in input_lines:\n",
                "            encoded_sentence = [CHAR2ID[char] for char in sentence if char in CHAR2ID]\n",
                "\n",
                "            if len(encoded_sentence) == 0:\n",
                "                output_list.append(\"\")\n",
                "                continue\n",
                "\n",
                "            predictions = predict(model, encoded_sentence)\n",
                "\n",
                "            diacritized_sentence = \"\"\n",
                "            pred_idx = 0\n",
                "            for char in sentence:\n",
                "                if char in CHAR2ID:\n",
                "                    diacritic_id = int(predictions[pred_idx])\n",
                "                    diacritic = ID2DIACRITIC[diacritic_id]\n",
                "                    pred_idx += 1\n",
                "\n",
                "                    # Only output for Arabic letters that are in target IDs\n",
                "                    if char in ARABIC_LETTERS:\n",
                "                        if current_id in target_ids:\n",
                "                            output_csv.append([current_id, diacritic_id])\n",
                "                        current_id += 1\n",
                "\n",
                "                    diacritized_sentence += char + diacritic\n",
                "                else:\n",
                "                    diacritized_sentence += char\n",
                "\n",
                "            output_list.append(diacritized_sentence.strip())\n",
                "    else:\n",
                "        # Original text file format\n",
                "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
                "            input_data = f.readlines()\n",
                "\n",
                "        output_list = []\n",
                "        output_csv = [[\"ID\", \"label\"]]\n",
                "        current_id = 0\n",
                "\n",
                "        model.eval()\n",
                "        for sentence in input_data:\n",
                "            encoded_sentence = [CHAR2ID[char] for char in sentence if char in CHAR2ID]\n",
                "\n",
                "            if len(encoded_sentence) == 0:\n",
                "                output_list.append(\"\")\n",
                "                continue\n",
                "\n",
                "            predictions = predict(model, encoded_sentence)\n",
                "\n",
                "            diacritized_sentence = \"\"\n",
                "            pred_idx = 0\n",
                "            for char in sentence:\n",
                "                if char in CHAR2ID:\n",
                "                    diacritic_id = int(predictions[pred_idx])\n",
                "                    diacritic = ID2DIACRITIC[diacritic_id]\n",
                "                    pred_idx += 1\n",
                "                    if char in ARABIC_LETTERS:\n",
                "                        output_csv.append([current_id, diacritic_id])\n",
                "                        current_id += 1\n",
                "                    diacritized_sentence += char + diacritic\n",
                "                else:\n",
                "                    diacritized_sentence += char\n",
                "\n",
                "            output_list.append(diacritized_sentence.strip())\n",
                "\n",
                "    # Write diacritized text output\n",
                "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
                "        for line in output_list:\n",
                "            f.write(line + \"\\n\")\n",
                "\n",
                "    # Write CSV submission output\n",
                "    output_path_csv = os.path.splitext(output_path)[0] + \".csv\"\n",
                "    with open(output_path_csv, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
                "        writer = csv.writer(file)\n",
                "        writer.writerows(output_csv)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "03fc9994",
            "metadata": {},
            "outputs": [],
            "source": [
                "train_dataset = generate_dataset(\"ArabicDataset\", \"../data/train.txt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "8eba76fb",
            "metadata": {},
            "outputs": [],
            "source": [
                "val_dataset = generate_dataset(\"ArabicDataset\", \"../data/val.txt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "ef3213c5",
            "metadata": {},
            "outputs": [],
            "source": [
                "model = generate_model(\n",
                "    model_name=\"BiLSTMCRFArabicModel\",\n",
                "    vocab_size=len(CHAR2ID),\n",
                "    embedding_dim=EMBEDDING_DIM,\n",
                "    hidden_dim=HIDDEN_DIM,\n",
                "    output_dim=len(DIACRITIC2ID),\n",
                "    PAD=PAD\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "2daacada",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<All keys matched successfully>"
                        ]
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Uncomment to load pre-trained model\n",
                "model_state_dict = torch.load(model_path, map_location=DEVICE)\n",
                "model.load_state_dict(model_state_dict)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "94e5422c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_506294/1971236994.py:24: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
                        "  scaler = GradScaler()  # For mixed precision training\n",
                        "Training Epoch 1:   0%|          | 0/1456 [00:00<?, ?it/s]/tmp/ipykernel_506294/1971236994.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
                        "  with autocast():\n",
                        "Training Epoch 1: 100%|██████████| 1456/1456 [17:15<00:00,  1.41it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1 | Train Loss: 3.0516\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Validation Epoch 1: 100%|██████████| 71/71 [00:14<00:00,  5.02it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1 | Validation Accuracy: 97.80%\n",
                        "  → New best model saved! (Accuracy: 97.80%)\n",
                        "\n",
                        "Training completed!\n",
                        "Best validation accuracy: 97.80%\n",
                        "Model saved to ../models/ArabicBiLSTMCRFModel.pth\n"
                    ]
                }
            ],
            "source": [
                "# Train with validation monitoring\n",
                "# patience=3: Stop if no improvement for 3 epochs (set to 0 to disable)\n",
                "# patience=0: Train for all NUM_EPOCHS without early stopping\n",
                "train(model, train_dataset, val_dataset, model_path, patience=3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "df45c57a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 71/71 [00:14<00:00,  5.05it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Validation Accuracy (Overall): 97.80%\n",
                        "Validation Accuracy (Without Last Character): 98.33%\n",
                        "Validation Accuracy (Last Character): 95.75%\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "evaluate(model, val_dataset)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "ba7d24bf",
            "metadata": {},
            "outputs": [],
            "source": [
                "infer(model, model_path, \"../input/test_no_diacritics.csv\", output_path)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
