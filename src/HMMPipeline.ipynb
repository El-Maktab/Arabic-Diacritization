{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f78b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "from typing import List, Any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cddf29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"../models/ArabicHMMModel.pkl\"\n",
    "input_path = f\"../input/dataset_no_diacritics.txt\"\n",
    "output_path = f\"../output/output.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c26ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configurations\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Global registries\n",
    "DATASET_REGISTRY: dict[str, Any] = {}\n",
    "MODEL_REGISTRY: dict[str, Any] = {}\n",
    "\n",
    "# Data parameters\n",
    "ARABIC_LETTERS = sorted(\n",
    "    np.load('../data/utils/arabic_letters.pkl', allow_pickle=True))\n",
    "DIACRITICS = sorted(np.load(\n",
    "    '../data/utils/diacritics.pkl', allow_pickle=True))\n",
    "PUNCTUATIONS = {\".\", \"،\", \":\", \"؛\", \"؟\", \"!\", '\"', \"-\"}\n",
    "\n",
    "VALID_CHARS = set(ARABIC_LETTERS).union(\n",
    "    set(DIACRITICS)).union(PUNCTUATIONS).union({\" \"})\n",
    "\n",
    "CHAR2ID = {char: id for id, char in enumerate(ARABIC_LETTERS)}\n",
    "CHAR2ID[\" \"] = len(ARABIC_LETTERS)\n",
    "CHAR2ID[\"<PAD>\"] = len(ARABIC_LETTERS) + 1\n",
    "PAD = CHAR2ID[\"<PAD>\"]\n",
    "SPACE = CHAR2ID[\" \"]\n",
    "ID2CHAR = {id: char for char, id in CHAR2ID.items()}\n",
    "\n",
    "DIACRITIC2ID = np.load('../data/utils/diacritic2id.pkl', allow_pickle=True)\n",
    "ID2DIACRITIC = {id: diacritic for diacritic, id in DIACRITIC2ID.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aa107dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def register_dataset(name):\n",
    "    def decorator(cls):\n",
    "        DATASET_REGISTRY[name] = cls\n",
    "        return cls\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def generate_dataset(dataset_name: str, *args, **kwargs):\n",
    "    try:\n",
    "        dataset_cls = DATASET_REGISTRY[dataset_name]\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Dataset '{dataset_name}' is not recognized.\")\n",
    "    return dataset_cls(*args, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def register_model(name):\n",
    "    def decorator(cls):\n",
    "        MODEL_REGISTRY[name] = cls\n",
    "        return cls\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def generate_model(model_name: str, *args, **kwargs):\n",
    "    try:\n",
    "        model_cls = MODEL_REGISTRY[model_name]\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Model '{model_name}' is not recognized.\")\n",
    "    return model_cls(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85a77ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@register_dataset(\"ArabicDataset\")\n",
    "class ArabicDataset(Dataset):\n",
    "    def __init__(self, file_path: str):\n",
    "        self.data_X, self.data_Y = self.generate_tensor_data(file_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_X[idx], self.data_Y[idx]\n",
    "\n",
    "    def generate_tensor_data(self, data_path: str):\n",
    "        data_Y = self.load_data(data_path)\n",
    "        data_X = self.extract_text_without_diacritics(data_Y)\n",
    "\n",
    "        encoded_data_X, encoded_data_Y = self.encode_data(data_X, data_Y)\n",
    "        data_X = torch.tensor(\n",
    "            encoded_data_X, dtype=torch.int64)\n",
    "        data_Y = torch.tensor(\n",
    "            encoded_data_Y, dtype=torch.int64)\n",
    "\n",
    "        return data_X, data_Y\n",
    "\n",
    "    def load_data(self, file_path: str):\n",
    "        data = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    line = re.sub(\n",
    "                        f'[^{re.escape(\"\".join(VALID_CHARS))}]', '', line)\n",
    "                    line = re.sub(r'\\s+', ' ', line)\n",
    "                    sentences = re.split(\n",
    "                        f'[{re.escape(\"\".join(PUNCTUATIONS))}]', line)\n",
    "                    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "                    data.extend(sentences)\n",
    "\n",
    "        return np.array(data)\n",
    "\n",
    "    def extract_text_without_diacritics(self, dataY):\n",
    "        dataX = dataY.copy()\n",
    "        for diacritic, _ in DIACRITIC2ID.items():\n",
    "            dataX = np.char.replace(\n",
    "                dataX, diacritic, '')\n",
    "        return dataX\n",
    "\n",
    "    def encode_data(self, dataX: List[str], dataY: List[str]):\n",
    "        encoded_data_X = []\n",
    "        for sentence in dataX:\n",
    "            encoded_data_X.append([CHAR2ID[char]\n",
    "                                   for char in sentence if char in CHAR2ID])\n",
    "        encoded_data_Y = []\n",
    "        for sentence in dataY:\n",
    "            encoded_data_Y.append(self.extract_diacritics(sentence))\n",
    "\n",
    "        max_sentence_len = max(len(sentence) for sentence in encoded_data_X)\n",
    "        padded_dataX = np.full(\n",
    "            (len(encoded_data_X), max_sentence_len), PAD, dtype=np.int64)\n",
    "        for i, seq in enumerate(encoded_data_X):\n",
    "            padded_dataX[i, :len(seq)] = seq\n",
    "\n",
    "        padded_dataY = np.full(\n",
    "            (len(encoded_data_Y), max_sentence_len), PAD, dtype=np.int64)\n",
    "        for i, seq in enumerate(encoded_data_Y):\n",
    "            padded_dataY[i, :len(seq)] = seq\n",
    "\n",
    "        return padded_dataX, padded_dataY\n",
    "\n",
    "    def extract_diacritics(self, sentence: str):\n",
    "        result = []\n",
    "        i = 0\n",
    "        n = len(sentence)\n",
    "        on_char = False\n",
    "\n",
    "        while i < n:\n",
    "            ch = sentence[i]\n",
    "            if ch in DIACRITICS:\n",
    "                on_char = False\n",
    "                # check if next char forms a stacked diacritic\n",
    "                if i+1 < n and sentence[i+1] in DIACRITICS:\n",
    "                    combined = ch + sentence[i+1]\n",
    "                    if combined in DIACRITIC2ID:\n",
    "                        result.append(DIACRITIC2ID[combined])\n",
    "                        i += 2\n",
    "                        continue\n",
    "                result.append(DIACRITIC2ID[ch])\n",
    "            elif ch in CHAR2ID:\n",
    "                if on_char:\n",
    "                    result.append(DIACRITIC2ID[''])\n",
    "                on_char = True\n",
    "            i += 1\n",
    "        if on_char:\n",
    "            result.append(DIACRITIC2ID[''])\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7300f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_model(\"HMMArabicModel\")\n",
    "class HMMArabicModel:\n",
    "    def __init__(self, num_states, num_observations, pad_state_id=None):\n",
    "        self.num_states = num_states\n",
    "        self.num_observations = num_observations\n",
    "        self.pad_state_id = pad_state_id\n",
    "        \n",
    "        self.log_pi = None             # (num_states,) initial state log probabilities\n",
    "        self.log_transition = None     # (num_states, num_states)\n",
    "        self.log_emission = None       # (num_states, num_observations)\n",
    "\n",
    "    def fit(self, seq_obs, seq_states, laplace=1.0):\n",
    "        \"\"\"\n",
    "        seq_obs: list of observation sequences (char ids)\n",
    "        seq_states: list of corresponding state sequences (diac ids)\n",
    "        laplace: add-k smoothing constant\n",
    "        \"\"\"\n",
    "        pi_counts = np.zeros(self.num_states, dtype=np.float64)\n",
    "        transition_counts = np.zeros((self.num_states, self.num_states), dtype=np.float64)\n",
    "        emission_counts = np.zeros((self.num_states, self.num_observations), dtype=np.float64)\n",
    "\n",
    "        total_sequences = 0\n",
    "        for obs, states in zip(seq_obs, seq_states):\n",
    "            #uniform length check\n",
    "            if len(obs) == 0 or len(obs) != len(states):\n",
    "                continue\n",
    "            total_sequences += 1\n",
    "\n",
    "            # initial state\n",
    "            s0 = states[0]\n",
    "            if self.is_pad(s0):\n",
    "                continue\n",
    "            pi_counts[s0] += 1.0\n",
    "            \n",
    "            # emissions and transitions\n",
    "            for i in range(len(obs)):\n",
    "                s = states[i]\n",
    "                o = obs[i]\n",
    "                if self.is_pad(s) or o is None:\n",
    "                    continue\n",
    "                emission_counts[s, o] += 1.0\n",
    "                if i + 1 < len(obs):\n",
    "                    s_next = states[i + 1]\n",
    "                    if not self.is_pad(s_next):\n",
    "                        transition_counts[s, s_next] += 1.0\n",
    "        # apply Laplace smoothing then normalize\n",
    "        pi_sm = pi_counts + laplace\n",
    "        self.log_pi = np.log(pi_sm / pi_sm.sum())\n",
    "\n",
    "        log_transition = transition_counts + laplace\n",
    "        transition_row_sums = log_transition.sum(axis=1, keepdims=True)\n",
    "        # avoid divide by zero\n",
    "        transition_row_sums[transition_row_sums == 0] = 1.0\n",
    "        self.log_transition = np.log(log_transition / transition_row_sums)\n",
    "\n",
    "        log_emission = emission_counts + laplace\n",
    "        emission_row_sums = log_emission.sum(axis=1, keepdims=True)\n",
    "        emission_row_sums[emission_row_sums == 0] = 1.0\n",
    "        self.log_emission = np.log(log_emission / emission_row_sums)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def is_pad(self, state_id):\n",
    "        return self.pad_state_id is not None and state_id == self.pad_state_id\n",
    "\n",
    "    def viterbi(self, obs_seq):\n",
    "        \"\"\"\n",
    "        obs_seq: list of observation ids (ints)\n",
    "        returns: best state sequence (list of state ids)\n",
    "        \"\"\"\n",
    "        T = len(obs_seq)\n",
    "        N = self.num_states\n",
    "        if T == 0:\n",
    "            return []\n",
    "\n",
    "        # Use -inf for impossible\n",
    "        neginf = -1e300\n",
    "\n",
    "        # delta[t, i] = max log prob of a path ending in state i at time t\n",
    "        delta = np.full((T, N), neginf, dtype=np.float64)\n",
    "        psi = np.zeros((T, N), dtype=np.int32)\n",
    "\n",
    "        # init\n",
    "        o0 = obs_seq[0]\n",
    "        # if observation index out of range, treat emission log-prob as neginf\n",
    "        emit0 = self.log_emission[:, o0] if 0 <= o0 < self.num_observations else np.full(N, neginf)\n",
    "        delta[0, :] = self.log_pi + emit0\n",
    "        psi[0, :] = 0\n",
    "\n",
    "        for t in range(1, T):\n",
    "            ot = obs_seq[t]\n",
    "            emit_t = self.log_emission[:, ot] if 0 <= ot < self.num_observations else np.full(N, neginf)\n",
    "            for j in range(N):\n",
    "                # compute delta[t-1, i] + logA[i,j] for all i\n",
    "                scores = delta[t-1, :] + self.log_transition[:, j]\n",
    "                i_max = np.argmax(scores)\n",
    "                delta[t, j] = scores[i_max] + emit_t[j]\n",
    "                psi[t, j] = i_max\n",
    "\n",
    "        # backtrack\n",
    "        states = [0] * T\n",
    "        states[T-1] = int(np.argmax(delta[T-1, :]))\n",
    "        for t in range(T-2, -1, -1):\n",
    "            states[t] = int(psi[t+1, states[t+1]])\n",
    "        return states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97275e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_val, Y_val):\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "    total_correct_ending = 0\n",
    "    total_tokens_ending = 0\n",
    "    total_correct_without_ending = 0\n",
    "    total_tokens_without_ending = 0\n",
    "\n",
    "    for obs_seq, true_seq in zip(X_val, Y_val):\n",
    "        pred_seq = model.viterbi(obs_seq)\n",
    "\n",
    "        # Determine end-of-word positions\n",
    "        end_of_word_mask = []\n",
    "        for i, o in enumerate(obs_seq):\n",
    "            if i + 1 < len(obs_seq):\n",
    "                end_of_word_mask.append(obs_seq[i + 1] == SPACE)\n",
    "            else:\n",
    "                end_of_word_mask.append(True)\n",
    "        end_of_word_mask = [bool(x) for x in end_of_word_mask]\n",
    "\n",
    "        for i, (pred, true) in enumerate(zip(pred_seq, true_seq)):\n",
    "            if true == PAD:\n",
    "                continue\n",
    "\n",
    "            total_tokens += 1\n",
    "            if pred == true:\n",
    "                total_correct += 1\n",
    "\n",
    "            if end_of_word_mask[i]:\n",
    "                total_tokens_ending += 1\n",
    "                if pred == true:\n",
    "                    total_correct_ending += 1\n",
    "            else:\n",
    "                total_tokens_without_ending += 1\n",
    "                if pred == true:\n",
    "                    total_correct_without_ending += 1\n",
    "\n",
    "    val_accuracy = total_correct / total_tokens * 100 if total_tokens else 0\n",
    "    val_accuracy_ending = total_correct_ending / total_tokens_ending * 100 if total_tokens_ending else 0\n",
    "    val_accuracy_without_ending = total_correct_without_ending / total_tokens_without_ending * 100 if total_tokens_without_ending else 0\n",
    "\n",
    "    print(\n",
    "        f\"Validation Accuracy (Overall): {val_accuracy:.2f}%\\n\" +\n",
    "        f\"Validation Accuracy (Without Last Character): {val_accuracy_without_ending:.2f}%\\n\" +\n",
    "        f\"Validation Accuracy (Last Character): {val_accuracy_ending:.2f}%\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc86498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_sequences(tensor):\n",
    "    sequences = []\n",
    "    for row in tensor:\n",
    "        seq = [int(x) for x in row if int(x) != PAD]\n",
    "        sequences.append(seq)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc9994",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = generate_dataset(\"ArabicDataset\", \"../data/train.txt\")\n",
    "X_train = tensor_to_sequences(train_dataset.data_X)\n",
    "Y_train = tensor_to_sequences(train_dataset.data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eba76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = generate_dataset(\"ArabicDataset\", \"../data/val.txt\")\n",
    "X_val = tensor_to_sequences(val_dataset.data_X)\n",
    "Y_val = tensor_to_sequences(val_dataset.data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3213c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HMM\n",
    "model = generate_model(\n",
    "    model_name=\"HMMArabicModel\",\n",
    "    num_states=len(DIACRITIC2ID),\n",
    "    num_observations=len(CHAR2ID),\n",
    "    pad_state_id=PAD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e5422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "print(f\"HMM model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e5d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "print(f\"HMM model loaded from {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df45c57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, X_val, Y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
