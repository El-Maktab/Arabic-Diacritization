{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "3f78b250",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1: Imports\n",
                "import numpy as np\n",
                "import re\n",
                "import os\n",
                "import csv\n",
                "import pickle\n",
                "from tqdm import tqdm\n",
                "from typing import List, Any, Dict, Tuple\n",
                "\n",
                "# Skip-gram (Word2Vec)\n",
                "from gensim.models import Word2Vec\n",
                "\n",
                "# CRF\n",
                "import sklearn_crfsuite\n",
                "from sklearn_crfsuite import metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "1cddf29e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Paths configuration\n",
                "model_path = \"../models/SkipgramCRFModel.pkl\"\n",
                "skipgram_model_path = \"../models/SkipgramWordEmbeddings.model\"\n",
                "input_path = \"../input/test_no_diacritics.txt\"\n",
                "output_path = \"../output/output_crf.txt\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "9c26ec0b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: Constants and hyperparameters\n",
                "\n",
                "# Global registries\n",
                "DATASET_REGISTRY: dict[str, Any] = {}\n",
                "MODEL_REGISTRY: dict[str, Any] = {}\n",
                "\n",
                "# Skip-gram hyperparameters\n",
                "EMBEDDING_DIM = 100\n",
                "WINDOW_SIZE = 5\n",
                "MIN_COUNT = 1\n",
                "SG = 1  # 1 for Skip-gram, 0 for CBOW\n",
                "WORKERS = 4\n",
                "SKIPGRAM_EPOCHS = 10\n",
                "\n",
                "# CRF hyperparameters\n",
                "CRF_ALGORITHM = 'lbfgs'\n",
                "CRF_C1 = 0.1  # L1 regularization\n",
                "CRF_C2 = 0.1  # L2 regularization\n",
                "CRF_MAX_ITERATIONS = 100\n",
                "\n",
                "# Context window for CRF features\n",
                "CONTEXT_WINDOW = 2\n",
                "\n",
                "# Data parameters\n",
                "ARABIC_LETTERS = sorted(\n",
                "    np.load('../data/utils/arabic_letters.pkl', allow_pickle=True))\n",
                "DIACRITICS = sorted(np.load(\n",
                "    '../data/utils/diacritics.pkl', allow_pickle=True))\n",
                "PUNCTUATIONS = {\".\", \"،\", \":\", \"؛\", \"؟\", \"!\", '\"', \"-\"}\n",
                "\n",
                "VALID_CHARS = set(ARABIC_LETTERS).union(\n",
                "    set(DIACRITICS)).union(PUNCTUATIONS).union({\" \"})\n",
                "\n",
                "CHAR2ID = {char: id for id, char in enumerate(ARABIC_LETTERS)}\n",
                "CHAR2ID[\" \"] = len(ARABIC_LETTERS)\n",
                "CHAR2ID[\"<PAD>\"] = len(ARABIC_LETTERS) + 1\n",
                "PAD = CHAR2ID[\"<PAD>\"]\n",
                "SPACE = CHAR2ID[\" \"]\n",
                "ID2CHAR = {id: char for char, id in CHAR2ID.items()}\n",
                "\n",
                "DIACRITIC2ID = np.load('../data/utils/diacritic2id.pkl', allow_pickle=True)\n",
                "ID2DIACRITIC = {id: diacritic for diacritic, id in DIACRITIC2ID.items()}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "7aa107dd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: Registry functions\n",
                "\n",
                "def register_dataset(name):\n",
                "    def decorator(cls):\n",
                "        DATASET_REGISTRY[name] = cls\n",
                "        return cls\n",
                "    return decorator\n",
                "\n",
                "\n",
                "def generate_dataset(dataset_name: str, *args, **kwargs):\n",
                "    try:\n",
                "        dataset_cls = DATASET_REGISTRY[dataset_name]\n",
                "    except KeyError:\n",
                "        raise ValueError(f\"Dataset '{dataset_name}' is not recognized.\")\n",
                "    return dataset_cls(*args, **kwargs)\n",
                "\n",
                "\n",
                "def register_model(name):\n",
                "    def decorator(cls):\n",
                "        MODEL_REGISTRY[name] = cls\n",
                "        return cls\n",
                "    return decorator\n",
                "\n",
                "\n",
                "def generate_model(model_name: str, *args, **kwargs):\n",
                "    try:\n",
                "        model_cls = MODEL_REGISTRY[model_name]\n",
                "    except KeyError:\n",
                "        raise ValueError(f\"Model '{model_name}' is not recognized.\")\n",
                "    return model_cls(*args, **kwargs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "85a77ff8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: Dataset class with Skip-gram feature extraction\n",
                "\n",
                "@register_dataset(\"ArabicSkipgramDataset\")\n",
                "class ArabicSkipgramDataset:\n",
                "    def __init__(self, file_path: str, skipgram_model: Word2Vec = None):\n",
                "        self.skipgram_model = skipgram_model\n",
                "        self.sentences_with_diacritics = self.load_data(file_path)\n",
                "        self.sentences_without_diacritics = self.extract_text_without_diacritics(\n",
                "            self.sentences_with_diacritics)\n",
                "        \n",
                "        # Tokenize into words for Skip-gram\n",
                "        self.tokenized_sentences = [sentence.split() for sentence in self.sentences_without_diacritics]\n",
                "        \n",
                "        # Extract diacritics per character\n",
                "        self.diacritics_per_sentence = [\n",
                "            self.extract_diacritics(sentence) \n",
                "            for sentence in self.sentences_with_diacritics\n",
                "        ]\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.sentences_without_diacritics)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        return self.tokenized_sentences[idx], self.diacritics_per_sentence[idx]\n",
                "\n",
                "    def load_data(self, file_path: str):\n",
                "        data = []\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            for line in f:\n",
                "                line = line.strip()\n",
                "                if line:\n",
                "                    line = re.sub(\n",
                "                        f'[^{re.escape(\"\".join(VALID_CHARS))}]', '', line)\n",
                "                    line = re.sub(r'\\s+', ' ', line)\n",
                "                    sentences = re.split(\n",
                "                        f'[{re.escape(\"\".join(PUNCTUATIONS))}]', line)\n",
                "                    sentences = [s.strip() for s in sentences if s.strip()]\n",
                "                    data.extend(sentences)\n",
                "        return np.array(data)\n",
                "\n",
                "    def extract_text_without_diacritics(self, dataY):\n",
                "        dataX = dataY.copy()\n",
                "        for diacritic, _ in DIACRITIC2ID.items():\n",
                "            dataX = np.char.replace(dataX, diacritic, '')\n",
                "        return dataX\n",
                "\n",
                "    def extract_diacritics(self, sentence: str):\n",
                "        \"\"\"Extract diacritics for each character in the sentence.\"\"\"\n",
                "        result = []\n",
                "        i = 0\n",
                "        n = len(sentence)\n",
                "        on_char = False\n",
                "\n",
                "        while i < n:\n",
                "            ch = sentence[i]\n",
                "            if ch in DIACRITICS:\n",
                "                on_char = False\n",
                "                if i+1 < n and sentence[i+1] in DIACRITICS:\n",
                "                    combined = ch + sentence[i+1]\n",
                "                    if combined in DIACRITIC2ID:\n",
                "                        result.append(str(DIACRITIC2ID[combined]))\n",
                "                        i += 2\n",
                "                        continue\n",
                "                result.append(str(DIACRITIC2ID[ch]))\n",
                "            elif ch in CHAR2ID:\n",
                "                if on_char:\n",
                "                    result.append(str(DIACRITIC2ID['']))\n",
                "                on_char = True\n",
                "            i += 1\n",
                "        if on_char:\n",
                "            result.append(str(DIACRITIC2ID['']))\n",
                "        return result\n",
                "\n",
                "    def get_corpus_for_skipgram(self):\n",
                "        \"\"\"Return tokenized sentences for Skip-gram training.\"\"\"\n",
                "        return self.tokenized_sentences"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "6270c87e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: CRF Model class and Skip-gram training\n",
                "\n",
                "def train_skipgram_model(corpus: List[List[str]], save_path: str) -> Word2Vec:\n",
                "    \"\"\"Train a Skip-gram Word2Vec model on the corpus.\"\"\"\n",
                "    print(\"Training Skip-gram model...\")\n",
                "    model = Word2Vec(\n",
                "        sentences=corpus,\n",
                "        vector_size=EMBEDDING_DIM,\n",
                "        window=WINDOW_SIZE,\n",
                "        min_count=MIN_COUNT,\n",
                "        sg=SG,\n",
                "        workers=WORKERS,\n",
                "        epochs=SKIPGRAM_EPOCHS\n",
                "    )\n",
                "    model.save(save_path)\n",
                "    print(f\"Skip-gram model saved to {save_path}\")\n",
                "    return model\n",
                "\n",
                "\n",
                "def load_skipgram_model(path: str) -> Word2Vec:\n",
                "    \"\"\"Load a pre-trained Skip-gram model.\"\"\"\n",
                "    return Word2Vec.load(path)\n",
                "\n",
                "\n",
                "def word_to_features(skipgram_model: Word2Vec, sentence: List[str], char_idx: int, word_idx: int, char_in_word: str, is_last_char: bool) -> Dict[str, Any]:\n",
                "    \"\"\"\n",
                "    Extract features for a single character using Skip-gram word embeddings.\n",
                "    \n",
                "    Features include:\n",
                "    - The word's embedding (if available)\n",
                "    - Character identity\n",
                "    - Position in word\n",
                "    - Context word embeddings\n",
                "    \"\"\"\n",
                "    features = {\n",
                "        'bias': 1.0,\n",
                "        'char': char_in_word,\n",
                "        'char_idx_in_sentence': char_idx,\n",
                "        'is_last_char_in_word': is_last_char,\n",
                "    }\n",
                "    \n",
                "    # Current word embedding\n",
                "    current_word = sentence[word_idx]\n",
                "    if current_word in skipgram_model.wv:\n",
                "        embedding = skipgram_model.wv[current_word]\n",
                "        for i, val in enumerate(embedding):\n",
                "            features[f'word_emb_{i}'] = float(val)\n",
                "    else:\n",
                "        for i in range(EMBEDDING_DIM):\n",
                "            features[f'word_emb_{i}'] = 0.0\n",
                "    \n",
                "    # Previous word embedding (context)\n",
                "    if word_idx > 0:\n",
                "        prev_word = sentence[word_idx - 1]\n",
                "        if prev_word in skipgram_model.wv:\n",
                "            embedding = skipgram_model.wv[prev_word]\n",
                "            for i, val in enumerate(embedding):\n",
                "                features[f'prev_word_emb_{i}'] = float(val)\n",
                "        else:\n",
                "            for i in range(EMBEDDING_DIM):\n",
                "                features[f'prev_word_emb_{i}'] = 0.0\n",
                "    else:\n",
                "        features['BOS'] = True  # Beginning of sentence\n",
                "        for i in range(EMBEDDING_DIM):\n",
                "            features[f'prev_word_emb_{i}'] = 0.0\n",
                "    \n",
                "    # Next word embedding (context)\n",
                "    if word_idx < len(sentence) - 1:\n",
                "        next_word = sentence[word_idx + 1]\n",
                "        if next_word in skipgram_model.wv:\n",
                "            embedding = skipgram_model.wv[next_word]\n",
                "            for i, val in enumerate(embedding):\n",
                "                features[f'next_word_emb_{i}'] = float(val)\n",
                "        else:\n",
                "            for i in range(EMBEDDING_DIM):\n",
                "                features[f'next_word_emb_{i}'] = 0.0\n",
                "    else:\n",
                "        features['EOS'] = True  # End of sentence\n",
                "        for i in range(EMBEDDING_DIM):\n",
                "            features[f'next_word_emb_{i}'] = 0.0\n",
                "    \n",
                "    return features\n",
                "\n",
                "\n",
                "def sentence_to_features(skipgram_model: Word2Vec, tokenized_sentence: List[str], sentence_without_diacritics: str) -> List[Dict[str, Any]]:\n",
                "    \"\"\"\n",
                "    Convert an entire sentence to CRF features.\n",
                "    Each character gets features based on its word's Skip-gram embedding.\n",
                "    \"\"\"\n",
                "    features = []\n",
                "    char_idx = 0\n",
                "    \n",
                "    for word_idx, word in enumerate(tokenized_sentence):\n",
                "        for i, char in enumerate(word):\n",
                "            if char in CHAR2ID and char != ' ':\n",
                "                is_last_char = (i == len(word) - 1)\n",
                "                feat = word_to_features(\n",
                "                    skipgram_model, tokenized_sentence, char_idx, word_idx, char, is_last_char\n",
                "                )\n",
                "                features.append(feat)\n",
                "                char_idx += 1\n",
                "    \n",
                "    return features\n",
                "\n",
                "\n",
                "@register_model(\"CRFArabicModel\")\n",
                "class CRFArabicModel:\n",
                "    def __init__(self):\n",
                "        self.crf = sklearn_crfsuite.CRF(\n",
                "            algorithm=CRF_ALGORITHM,\n",
                "            c1=CRF_C1,\n",
                "            c2=CRF_C2,\n",
                "            max_iterations=CRF_MAX_ITERATIONS,\n",
                "            all_possible_transitions=True\n",
                "        )\n",
                "    \n",
                "    def fit(self, X_train: List[List[Dict]], y_train: List[List[str]]):\n",
                "        \"\"\"Train the CRF model.\"\"\"\n",
                "        self.crf.fit(X_train, y_train)\n",
                "    \n",
                "    def predict(self, X: List[List[Dict]]) -> List[List[str]]:\n",
                "        \"\"\"Predict diacritics for input sequences.\"\"\"\n",
                "        return self.crf.predict(X)\n",
                "    \n",
                "    def save(self, path: str):\n",
                "        \"\"\"Save the CRF model to disk.\"\"\"\n",
                "        with open(path, 'wb') as f:\n",
                "            pickle.dump(self.crf, f)\n",
                "    \n",
                "    def load(self, path: str):\n",
                "        \"\"\"Load a CRF model from disk.\"\"\"\n",
                "        with open(path, 'rb') as f:\n",
                "            self.crf = pickle.load(f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "a782bf2f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7: Train function\n",
                "\n",
                "def train(model: CRFArabicModel, train_dataset: ArabicSkipgramDataset, \n",
                "          skipgram_model: Word2Vec, model_path: str):\n",
                "    \"\"\"\n",
                "    Train the CRF model on the training dataset.\n",
                "    \"\"\"\n",
                "    print(\"Preparing training data for CRF...\")\n",
                "    X_train = []\n",
                "    y_train = []\n",
                "    \n",
                "    for idx in tqdm(range(len(train_dataset)), desc=\"Extracting features\"):\n",
                "        tokenized_sentence, diacritics = train_dataset[idx]\n",
                "        sentence_without_diacritics = train_dataset.sentences_without_diacritics[idx]\n",
                "        \n",
                "        features = sentence_to_features(\n",
                "            skipgram_model, tokenized_sentence, sentence_without_diacritics\n",
                "        )\n",
                "        \n",
                "        # Ensure features and diacritics have the same length\n",
                "        if len(features) == len(diacritics):\n",
                "            X_train.append(features)\n",
                "            y_train.append(diacritics)\n",
                "    \n",
                "    print(f\"Training CRF on {len(X_train)} sentences...\")\n",
                "    model.fit(X_train, y_train)\n",
                "    model.save(model_path)\n",
                "    print(f\"CRF model saved to {model_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "f00cc728",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 8: Evaluate function\n",
                "\n",
                "def evaluate(model: CRFArabicModel, val_dataset: ArabicSkipgramDataset, \n",
                "             skipgram_model: Word2Vec):\n",
                "    \"\"\"\n",
                "    Evaluate the CRF model on the validation dataset.\n",
                "    Reports accuracy for:\n",
                "    - Overall\n",
                "    - Without last character (morphology)\n",
                "    - Last character only (syntax/case endings)\n",
                "    \"\"\"\n",
                "    print(\"Preparing validation data...\")\n",
                "    X_val = []\n",
                "    y_val = []\n",
                "    last_char_indices = []  # Track which indices are last characters in words\n",
                "    \n",
                "    for idx in tqdm(range(len(val_dataset)), desc=\"Extracting validation features\"):\n",
                "        tokenized_sentence, diacritics = val_dataset[idx]\n",
                "        sentence_without_diacritics = val_dataset.sentences_without_diacritics[idx]\n",
                "        \n",
                "        features = sentence_to_features(\n",
                "            skipgram_model, tokenized_sentence, sentence_without_diacritics\n",
                "        )\n",
                "        \n",
                "        if len(features) == len(diacritics):\n",
                "            X_val.append(features)\n",
                "            y_val.append(diacritics)\n",
                "            \n",
                "            # Track last character positions\n",
                "            sentence_last_chars = [feat.get('is_last_char_in_word', False) for feat in features]\n",
                "            last_char_indices.append(sentence_last_chars)\n",
                "    \n",
                "    print(\"Running predictions...\")\n",
                "    y_pred = model.predict(X_val)\n",
                "    \n",
                "    # Calculate accuracies\n",
                "    total_correct = 0\n",
                "    total_tokens = 0\n",
                "    total_correct_ending = 0\n",
                "    total_tokens_ending = 0\n",
                "    total_correct_without_ending = 0\n",
                "    total_tokens_without_ending = 0\n",
                "    \n",
                "    for sent_idx in range(len(y_val)):\n",
                "        for token_idx in range(len(y_val[sent_idx])):\n",
                "            pred = y_pred[sent_idx][token_idx]\n",
                "            true = y_val[sent_idx][token_idx]\n",
                "            is_last_char = last_char_indices[sent_idx][token_idx]\n",
                "            \n",
                "            total_tokens += 1\n",
                "            if pred == true:\n",
                "                total_correct += 1\n",
                "            \n",
                "            if is_last_char:\n",
                "                total_tokens_ending += 1\n",
                "                if pred == true:\n",
                "                    total_correct_ending += 1\n",
                "            else:\n",
                "                total_tokens_without_ending += 1\n",
                "                if pred == true:\n",
                "                    total_correct_without_ending += 1\n",
                "    \n",
                "    val_accuracy = (total_correct / total_tokens) * 100 if total_tokens > 0 else 0\n",
                "    val_accuracy_ending = (total_correct_ending / total_tokens_ending) * 100 if total_tokens_ending > 0 else 0\n",
                "    val_accuracy_without_ending = (total_correct_without_ending / total_tokens_without_ending) * 100 if total_tokens_without_ending > 0 else 0\n",
                "    \n",
                "    print(f\"Validation Accuracy (Overall): {val_accuracy:.2f}%\")\n",
                "    print(f\"Validation Accuracy (Without Last Character): {val_accuracy_without_ending:.2f}%\")\n",
                "    print(f\"Validation Accuracy (Last Character): {val_accuracy_ending:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "3ed91e8d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 9: Predict function\n",
                "\n",
                "def predict(model: CRFArabicModel, skipgram_model: Word2Vec, \n",
                "            sentence: str) -> List[str]:\n",
                "    \"\"\"\n",
                "    Predict diacritics for a single sentence.\n",
                "    \"\"\"\n",
                "    # Remove any existing diacritics\n",
                "    clean_sentence = sentence\n",
                "    for diacritic in DIACRITICS:\n",
                "        clean_sentence = clean_sentence.replace(diacritic, '')\n",
                "    \n",
                "    tokenized = clean_sentence.split()\n",
                "    features = sentence_to_features(skipgram_model, tokenized, clean_sentence)\n",
                "    \n",
                "    if not features:\n",
                "        return []\n",
                "    \n",
                "    predictions = model.predict([features])[0]\n",
                "    return predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "02a98acc",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 10: Infer function\n",
                "\n",
                "def infer(model: CRFArabicModel, skipgram_model: Word2Vec,\n",
                "          input_path: str, output_path: str):\n",
                "    \"\"\"\n",
                "    Run inference on an input file and save results.\n",
                "    \"\"\"\n",
                "    with open(input_path, 'r', encoding='utf-8') as f:\n",
                "        input_data = f.readlines()\n",
                "    \n",
                "    output_list = []\n",
                "    output_csv = [[\"ID\", \"Label\"]]\n",
                "    current_id = 0\n",
                "    \n",
                "    for sentence in tqdm(input_data, desc=\"Inference\"):\n",
                "        sentence = sentence.strip()\n",
                "        if not sentence:\n",
                "            output_list.append(\"\")\n",
                "            continue\n",
                "        \n",
                "        # Clean sentence\n",
                "        clean_sentence = sentence\n",
                "        for diacritic in DIACRITICS:\n",
                "            clean_sentence = clean_sentence.replace(diacritic, '')\n",
                "        \n",
                "        predictions = predict(model, skipgram_model, clean_sentence)\n",
                "        \n",
                "        # Reconstruct diacritized sentence\n",
                "        diacritized_sentence = \"\"\n",
                "        pred_idx = 0\n",
                "        for char in clean_sentence:\n",
                "            diacritized_sentence += char\n",
                "            if char in ARABIC_LETTERS and pred_idx < len(predictions):\n",
                "                diacritic_id = int(predictions[pred_idx])\n",
                "                diacritic = ID2DIACRITIC.get(diacritic_id, '')\n",
                "                diacritized_sentence += diacritic\n",
                "                output_csv.append([current_id, diacritic_id])\n",
                "                current_id += 1\n",
                "                pred_idx += 1\n",
                "        \n",
                "        output_list.append(diacritized_sentence)\n",
                "    \n",
                "    with open(output_path, 'w', encoding='utf-8') as f:\n",
                "        for line in output_list:\n",
                "            f.write(line + '\\n')\n",
                "    \n",
                "    output_path_csv = os.path.splitext(output_path)[0] + \".csv\"\n",
                "    with open(output_path_csv, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
                "        writer = csv.writer(file)\n",
                "        writer.writerows(output_csv)\n",
                "    \n",
                "    print(f\"Output saved to {output_path} and {output_path_csv}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "03fc9994",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 11: Load training dataset\n",
                "train_dataset = generate_dataset(\"ArabicSkipgramDataset\", \"../data/train.txt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "8eba76fb",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training Skip-gram model...\n",
                        "Skip-gram model saved to ../models/SkipgramWordEmbeddings.model\n"
                    ]
                }
            ],
            "source": [
                "# Cell 12: Train Skip-gram model on training corpus\n",
                "corpus = train_dataset.get_corpus_for_skipgram()\n",
                "skipgram_model = train_skipgram_model(corpus, skipgram_model_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "ef3213c5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 13: Create CRF model\n",
                "crf_model = generate_model(\"CRFArabicModel\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "94e5422c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Preparing training data for CRF...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Extracting features: 100%|██████████| 186315/186315 [05:21<00:00, 579.79it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training CRF on 20019 sentences...\n",
                        "CRF model saved to ../models/SkipgramCRFModel.pkl\n"
                    ]
                }
            ],
            "source": [
                "# Cell 14: Train CRF model\n",
                "train(crf_model, train_dataset, skipgram_model, model_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "2daacada",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 15: Load validation dataset\n",
                "val_dataset = generate_dataset(\"ArabicSkipgramDataset\", \"../data/val.txt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "df45c57a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Preparing validation data...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Extracting validation features: 100%|██████████| 9068/9068 [00:15<00:00, 592.02it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running predictions...\n",
                        "Validation Accuracy (Overall): 82.33%\n",
                        "Validation Accuracy (Without Last Character): 81.87%\n",
                        "Validation Accuracy (Last Character): 83.82%\n"
                    ]
                }
            ],
            "source": [
                "# Cell 16: Evaluate the model\n",
                "evaluate(crf_model, val_dataset, skipgram_model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "ba7d24bf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 17: Run inference (optional)\n",
                "# infer(crf_model, skipgram_model, input_path, output_path)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
