{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "3f78b250",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1: Imports\n",
                "import numpy as np\n",
                "import re\n",
                "import os\n",
                "import csv\n",
                "import pickle\n",
                "from tqdm import tqdm\n",
                "from typing import List, Any, Dict, Tuple\n",
                "\n",
                "# PyTorch for CNN and Bi-LSTM\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "# Skip-gram (Word2Vec)\n",
                "from gensim.models import Word2Vec\n",
                "\n",
                "# PyTorch CRF\n",
                "from torchcrf import CRF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "1cddf29e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Paths configuration\n",
                "model_path = \"../models/BiLSTMCRFModel.pth\"\n",
                "skipgram_model_path = \"../models/SkipgramWordEmbeddings.model\"\n",
                "cnn_model_path = \"../models/CNNCharEncoder.pth\"\n",
                "cnn_cache_path = \"../models/cnn_word_embeddings_cache.pkl\"\n",
                "input_path = \"../input/test_no_diacritics.txt\"\n",
                "output_path = \"../output/output_bilstm_crf.txt\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "9c26ec0b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n",
                        "Number of diacritic classes: 15\n"
                    ]
                }
            ],
            "source": [
                "# Cell 3: Constants and hyperparameters\n",
                "\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {DEVICE}\")\n",
                "\n",
                "# Global registries\n",
                "DATASET_REGISTRY: dict[str, Any] = {}\n",
                "MODEL_REGISTRY: dict[str, Any] = {}\n",
                "\n",
                "# Skip-gram hyperparameters\n",
                "SKIPGRAM_EMBEDDING_DIM = 100\n",
                "WINDOW_SIZE = 5\n",
                "MIN_COUNT = 1\n",
                "SG = 1  # 1 for Skip-gram, 0 for CBOW\n",
                "WORKERS = 4\n",
                "SKIPGRAM_EPOCHS = 10\n",
                "\n",
                "# CNN hyperparameters\n",
                "CNN_CHAR_EMBEDDING_DIM = 30\n",
                "CNN_NUM_FILTERS = 50\n",
                "CNN_KERNEL_SIZES = [2, 3, 4]  # n-gram sizes\n",
                "CNN_OUTPUT_DIM = CNN_NUM_FILTERS * len(CNN_KERNEL_SIZES)  # 150\n",
                "CNN_BATCH_SIZE = 512  # Batch size for CNN inference\n",
                "\n",
                "# Bi-LSTM hyperparameters\n",
                "BILSTM_INPUT_DIM = SKIPGRAM_EMBEDDING_DIM + CNN_OUTPUT_DIM  # 100 + 150 = 250\n",
                "BILSTM_HIDDEN_DIM = 256\n",
                "BILSTM_NUM_LAYERS = 2\n",
                "BILSTM_DROPOUT = 0.3\n",
                "\n",
                "# Training hyperparameters\n",
                "BATCH_SIZE = 32\n",
                "NUM_EPOCHS = 10\n",
                "LEARNING_RATE = 0.001\n",
                "\n",
                "# Data parameters\n",
                "ARABIC_LETTERS = sorted(\n",
                "    np.load('../data/utils/arabic_letters.pkl', allow_pickle=True))\n",
                "DIACRITICS = sorted(np.load(\n",
                "    '../data/utils/diacritics.pkl', allow_pickle=True))\n",
                "PUNCTUATIONS = {\".\", \"،\", \":\", \"؛\", \"؟\", \"!\", '\"', \"-\"}\n",
                "\n",
                "VALID_CHARS = set(ARABIC_LETTERS).union(\n",
                "    set(DIACRITICS)).union(PUNCTUATIONS).union({\" \"})\n",
                "\n",
                "CHAR2ID = {char: id for id, char in enumerate(ARABIC_LETTERS)}\n",
                "CHAR2ID[\" \"] = len(ARABIC_LETTERS)\n",
                "CHAR2ID[\"<PAD>\"] = len(ARABIC_LETTERS) + 1\n",
                "PAD = CHAR2ID[\"<PAD>\"]\n",
                "SPACE = CHAR2ID[\" \"]\n",
                "ID2CHAR = {id: char for char, id in CHAR2ID.items()}\n",
                "VOCAB_SIZE = len(CHAR2ID)\n",
                "\n",
                "DIACRITIC2ID = np.load('../data/utils/diacritic2id.pkl', allow_pickle=True)\n",
                "ID2DIACRITIC = {id: diacritic for diacritic, id in DIACRITIC2ID.items()}\n",
                "NUM_TAGS = len(DIACRITIC2ID)\n",
                "print(f\"Number of diacritic classes: {NUM_TAGS}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "7aa107dd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: Registry functions\n",
                "\n",
                "def register_dataset(name):\n",
                "    def decorator(cls):\n",
                "        DATASET_REGISTRY[name] = cls\n",
                "        return cls\n",
                "    return decorator\n",
                "\n",
                "\n",
                "def generate_dataset(dataset_name: str, *args, **kwargs):\n",
                "    try:\n",
                "        dataset_cls = DATASET_REGISTRY[dataset_name]\n",
                "    except KeyError:\n",
                "        raise ValueError(f\"Dataset '{dataset_name}' is not recognized.\")\n",
                "    return dataset_cls(*args, **kwargs)\n",
                "\n",
                "\n",
                "def register_model(name):\n",
                "    def decorator(cls):\n",
                "        MODEL_REGISTRY[name] = cls\n",
                "        return cls\n",
                "    return decorator\n",
                "\n",
                "\n",
                "def generate_model(model_name: str, *args, **kwargs):\n",
                "    try:\n",
                "        model_cls = MODEL_REGISTRY[model_name]\n",
                "    except KeyError:\n",
                "        raise ValueError(f\"Model '{model_name}' is not recognized.\")\n",
                "    return model_cls(*args, **kwargs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "85a77ff8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: Dataset class\n",
                "\n",
                "@register_dataset(\"ArabicBiLSTMDataset\")\n",
                "class ArabicBiLSTMDataset:\n",
                "    def __init__(self, file_path: str, skipgram_model: Word2Vec = None):\n",
                "        self.skipgram_model = skipgram_model\n",
                "        self.sentences_with_diacritics = self.load_data(file_path)\n",
                "        self.sentences_without_diacritics = self.extract_text_without_diacritics(\n",
                "            self.sentences_with_diacritics)\n",
                "        \n",
                "        # Tokenize into words for Skip-gram\n",
                "        self.tokenized_sentences = [sentence.split() for sentence in self.sentences_without_diacritics]\n",
                "        \n",
                "        # Extract diacritics per character\n",
                "        self.diacritics_per_sentence = [\n",
                "            self.extract_diacritics(sentence) \n",
                "            for sentence in self.sentences_with_diacritics\n",
                "        ]\n",
                "        \n",
                "        # Collect all unique words for CNN batch processing\n",
                "        self.unique_words = set()\n",
                "        for sentence in self.tokenized_sentences:\n",
                "            self.unique_words.update(sentence)\n",
                "        self.unique_words = list(self.unique_words)\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.sentences_without_diacritics)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        return self.tokenized_sentences[idx], self.diacritics_per_sentence[idx]\n",
                "\n",
                "    def load_data(self, file_path: str):\n",
                "        data = []\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            for line in f:\n",
                "                line = line.strip()\n",
                "                if line:\n",
                "                    line = re.sub(\n",
                "                        f'[^{re.escape(\"\".join(VALID_CHARS))}]', '', line)\n",
                "                    line = re.sub(r'\\s+', ' ', line)\n",
                "                    sentences = re.split(\n",
                "                        f'[{re.escape(\"\".join(PUNCTUATIONS))}]', line)\n",
                "                    sentences = [s.strip() for s in sentences if s.strip()]\n",
                "                    data.extend(sentences)\n",
                "        return np.array(data)\n",
                "\n",
                "    def extract_text_without_diacritics(self, dataY):\n",
                "        dataX = dataY.copy()\n",
                "        for diacritic, _ in DIACRITIC2ID.items():\n",
                "            dataX = np.char.replace(dataX, diacritic, '')\n",
                "        return dataX\n",
                "\n",
                "    def extract_diacritics(self, sentence: str):\n",
                "        \"\"\"Extract diacritics for each character in the sentence.\"\"\"\n",
                "        result = []\n",
                "        i = 0\n",
                "        n = len(sentence)\n",
                "        on_char = False\n",
                "\n",
                "        while i < n:\n",
                "            ch = sentence[i]\n",
                "            if ch in DIACRITICS:\n",
                "                on_char = False\n",
                "                if i+1 < n and sentence[i+1] in DIACRITICS:\n",
                "                    combined = ch + sentence[i+1]\n",
                "                    if combined in DIACRITIC2ID:\n",
                "                        result.append(DIACRITIC2ID[combined])\n",
                "                        i += 2\n",
                "                        continue\n",
                "                result.append(DIACRITIC2ID[ch])\n",
                "            elif ch in CHAR2ID:\n",
                "                if on_char:\n",
                "                    result.append(DIACRITIC2ID[''])\n",
                "                on_char = True\n",
                "            i += 1\n",
                "        if on_char:\n",
                "            result.append(DIACRITIC2ID[''])\n",
                "        return result\n",
                "\n",
                "    def get_corpus_for_skipgram(self):\n",
                "        \"\"\"Return tokenized sentences for Skip-gram training.\"\"\"\n",
                "        return self.tokenized_sentences"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "6270c87e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: CNN Character Encoder\n",
                "\n",
                "class CNNCharEncoder(nn.Module):\n",
                "    \"\"\"\n",
                "    CNN-based character encoder that produces a fixed-size embedding for each word\n",
                "    based on its character sequence.\n",
                "    \"\"\"\n",
                "    def __init__(self, vocab_size, char_embedding_dim, num_filters, kernel_sizes):\n",
                "        super(CNNCharEncoder, self).__init__()\n",
                "        \n",
                "        self.char_embedding = nn.Embedding(vocab_size, char_embedding_dim, padding_idx=PAD)\n",
                "        \n",
                "        # Multiple CNN layers with different kernel sizes to capture different n-grams\n",
                "        self.convs = nn.ModuleList([\n",
                "            nn.Conv1d(in_channels=char_embedding_dim, \n",
                "                     out_channels=num_filters, \n",
                "                     kernel_size=k,\n",
                "                     padding=k//2)\n",
                "            for k in kernel_sizes\n",
                "        ])\n",
                "        \n",
                "        self.output_dim = num_filters * len(kernel_sizes)\n",
                "    \n",
                "    def forward(self, char_ids):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            char_ids: Tensor of shape (batch_size, max_word_len)\n",
                "        Returns:\n",
                "            word_embedding: Tensor of shape (batch_size, output_dim)\n",
                "        \"\"\"\n",
                "        # Embed characters: (batch, max_word_len, char_embedding_dim)\n",
                "        embedded = self.char_embedding(char_ids)\n",
                "        \n",
                "        # Transpose for Conv1d: (batch, char_embedding_dim, max_word_len)\n",
                "        embedded = embedded.transpose(1, 2)\n",
                "        \n",
                "        # Apply each conv layer and max-pool\n",
                "        conv_outputs = []\n",
                "        for conv in self.convs:\n",
                "            conv_out = F.relu(conv(embedded))  # (batch, num_filters, seq_len)\n",
                "            pooled = F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2)  # (batch, num_filters)\n",
                "            conv_outputs.append(pooled)\n",
                "        \n",
                "        # Concatenate all conv outputs\n",
                "        word_embedding = torch.cat(conv_outputs, dim=1)  # (batch, output_dim)\n",
                "        return word_embedding\n",
                "\n",
                "\n",
                "def word_to_char_ids(word: str, max_len: int = 20) -> List[int]:\n",
                "    \"\"\"Convert a word to a list of character IDs, padded to max_len.\"\"\"\n",
                "    char_ids = [CHAR2ID.get(c, PAD) for c in word if c in CHAR2ID]\n",
                "    # Pad or truncate\n",
                "    if len(char_ids) < max_len:\n",
                "        char_ids = char_ids + [PAD] * (max_len - len(char_ids))\n",
                "    else:\n",
                "        char_ids = char_ids[:max_len]\n",
                "    return char_ids"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "batch_cnn",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7: Batch CNN embedding computation with caching\n",
                "\n",
                "def compute_cnn_embeddings_batch(cnn_model: CNNCharEncoder, words: List[str], \n",
                "                                  batch_size: int = CNN_BATCH_SIZE) -> Dict[str, np.ndarray]:\n",
                "    \"\"\"\n",
                "    Compute CNN embeddings for all words in batch on GPU.\n",
                "    Returns a dictionary mapping word -> embedding.\n",
                "    \"\"\"\n",
                "    print(f\"Computing CNN embeddings for {len(words)} unique words on {DEVICE}...\")\n",
                "    cnn_model.eval()\n",
                "    cnn_model.to(DEVICE)\n",
                "    \n",
                "    word_to_embedding = {}\n",
                "    \n",
                "    # Process in batches\n",
                "    for i in tqdm(range(0, len(words), batch_size), desc=\"CNN batch processing\"):\n",
                "        batch_words = words[i:i+batch_size]\n",
                "        \n",
                "        # Convert words to char IDs\n",
                "        batch_char_ids = [word_to_char_ids(word) for word in batch_words]\n",
                "        batch_tensor = torch.tensor(batch_char_ids, dtype=torch.long).to(DEVICE)\n",
                "        \n",
                "        # Forward pass on GPU\n",
                "        with torch.no_grad():\n",
                "            embeddings = cnn_model(batch_tensor).cpu().numpy()\n",
                "        \n",
                "        # Store in cache\n",
                "        for word, emb in zip(batch_words, embeddings):\n",
                "            word_to_embedding[word] = emb\n",
                "    \n",
                "    print(f\"Computed {len(word_to_embedding)} CNN word embeddings\")\n",
                "    return word_to_embedding\n",
                "\n",
                "\n",
                "def save_cnn_cache(cache: Dict[str, np.ndarray], path: str):\n",
                "    \"\"\"Save CNN embedding cache to disk.\"\"\"\n",
                "    with open(path, 'wb') as f:\n",
                "        pickle.dump(cache, f)\n",
                "    print(f\"CNN cache saved to {path}\")\n",
                "\n",
                "\n",
                "def load_cnn_cache(path: str) -> Dict[str, np.ndarray]:\n",
                "    \"\"\"Load CNN embedding cache from disk.\"\"\"\n",
                "    with open(path, 'rb') as f:\n",
                "        cache = pickle.load(f)\n",
                "    print(f\"Loaded CNN cache with {len(cache)} words\")\n",
                "    return cache"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "cnn_skipgram",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 8: Skip-gram training and embedding functions\n",
                "\n",
                "def train_skipgram_model(corpus: List[List[str]], save_path: str) -> Word2Vec:\n",
                "    \"\"\"Train a Skip-gram Word2Vec model on the corpus.\"\"\"\n",
                "    print(\"Training Skip-gram model...\")\n",
                "    model = Word2Vec(\n",
                "        sentences=corpus,\n",
                "        vector_size=SKIPGRAM_EMBEDDING_DIM,\n",
                "        window=WINDOW_SIZE,\n",
                "        min_count=MIN_COUNT,\n",
                "        sg=SG,\n",
                "        workers=WORKERS,\n",
                "        epochs=SKIPGRAM_EPOCHS\n",
                "    )\n",
                "    model.save(save_path)\n",
                "    print(f\"Skip-gram model saved to {save_path}\")\n",
                "    return model\n",
                "\n",
                "\n",
                "def load_skipgram_model(path: str) -> Word2Vec:\n",
                "    \"\"\"Load a pre-trained Skip-gram model.\"\"\"\n",
                "    return Word2Vec.load(path)\n",
                "\n",
                "\n",
                "# Pre-cache Skip-gram lookups for speed\n",
                "SKIPGRAM_CACHE: Dict[str, np.ndarray] = {}\n",
                "\n",
                "def get_skipgram_embedding(skipgram_model: Word2Vec, word: str) -> np.ndarray:\n",
                "    \"\"\"Get Skip-gram embedding with caching.\"\"\"\n",
                "    if word not in SKIPGRAM_CACHE:\n",
                "        if word in skipgram_model.wv:\n",
                "            SKIPGRAM_CACHE[word] = skipgram_model.wv[word]\n",
                "        else:\n",
                "            SKIPGRAM_CACHE[word] = np.zeros(SKIPGRAM_EMBEDDING_DIM)\n",
                "    return SKIPGRAM_CACHE[word]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "bilstm_crf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 9: Bi-LSTM + CRF Model\n",
                "\n",
                "@register_model(\"BiLSTMCRFModel\")\n",
                "class BiLSTMCRFModel(nn.Module):\n",
                "    \"\"\"\n",
                "    Bi-LSTM + CRF Model for Arabic Diacritization.\n",
                "    \n",
                "    Architecture:\n",
                "    1. Input: Pre-computed embeddings (Skip-gram word + CNN char) per character\n",
                "    2. Bi-LSTM: Captures bidirectional context across the sequence\n",
                "    3. Linear: Projects LSTM hidden states to tag space\n",
                "    4. CRF: Models transition probabilities between diacritic tags\n",
                "    \n",
                "    The Bi-LSTM makes predictions, and the CRF layer validates them by:\n",
                "    - Learning which tag transitions are valid/likely\n",
                "    - Enforcing global sequence-level constraints during decoding\n",
                "    \"\"\"\n",
                "    def __init__(self, input_dim, hidden_dim, num_tags, num_layers=2, dropout=0.3):\n",
                "        super(BiLSTMCRFModel, self).__init__()\n",
                "        \n",
                "        self.hidden_dim = hidden_dim\n",
                "        self.num_tags = num_tags\n",
                "        \n",
                "        # Optional: Project input embeddings\n",
                "        self.input_projection = nn.Linear(input_dim, hidden_dim)\n",
                "        \n",
                "        # Bi-LSTM layer\n",
                "        self.lstm = nn.LSTM(\n",
                "            input_size=hidden_dim,\n",
                "            hidden_size=hidden_dim,\n",
                "            num_layers=num_layers,\n",
                "            batch_first=True,\n",
                "            bidirectional=True,\n",
                "            dropout=dropout if num_layers > 1 else 0\n",
                "        )\n",
                "        \n",
                "        # Linear layer to project LSTM output to tag space\n",
                "        # Bi-LSTM outputs hidden_dim * 2 (forward + backward)\n",
                "        self.hidden2tag = nn.Linear(hidden_dim * 2, num_tags)\n",
                "        \n",
                "        # CRF layer for sequence-level tag decoding\n",
                "        self.crf = CRF(num_tags, batch_first=True)\n",
                "        \n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "    \n",
                "    def _get_lstm_features(self, x):\n",
                "        \"\"\"\n",
                "        Get emission scores from Bi-LSTM.\n",
                "        \n",
                "        Args:\n",
                "            x: Input tensor of shape (batch_size, seq_len, input_dim)\n",
                "        Returns:\n",
                "            emissions: Tensor of shape (batch_size, seq_len, num_tags)\n",
                "        \"\"\"\n",
                "        # Project input\n",
                "        x = self.input_projection(x)\n",
                "        x = self.dropout(x)\n",
                "        \n",
                "        # Bi-LSTM\n",
                "        lstm_out, _ = self.lstm(x)\n",
                "        lstm_out = self.dropout(lstm_out)\n",
                "        \n",
                "        # Project to tag space\n",
                "        emissions = self.hidden2tag(lstm_out)\n",
                "        \n",
                "        return emissions\n",
                "    \n",
                "    def forward(self, x, tags, mask=None):\n",
                "        \"\"\"\n",
                "        Compute CRF loss for training.\n",
                "        \n",
                "        Args:\n",
                "            x: Input features (batch_size, seq_len, input_dim)\n",
                "            tags: True tags (batch_size, seq_len)\n",
                "            mask: Mask tensor (batch_size, seq_len), 1 for valid positions\n",
                "        Returns:\n",
                "            loss: Negative log-likelihood loss\n",
                "        \"\"\"\n",
                "        emissions = self._get_lstm_features(x)\n",
                "        \n",
                "        # CRF computes negative log-likelihood\n",
                "        # We negate it because CRF returns log-likelihood, and we want to minimize loss\n",
                "        loss = -self.crf(emissions, tags, mask=mask, reduction='mean')\n",
                "        \n",
                "        return loss\n",
                "    \n",
                "    def decode(self, x, mask=None):\n",
                "        \"\"\"\n",
                "        Decode the best tag sequence using Viterbi algorithm.\n",
                "        \n",
                "        Args:\n",
                "            x: Input features (batch_size, seq_len, input_dim)\n",
                "            mask: Mask tensor (batch_size, seq_len)\n",
                "        Returns:\n",
                "            best_tags: List of lists containing best tag sequences\n",
                "        \"\"\"\n",
                "        emissions = self._get_lstm_features(x)\n",
                "        \n",
                "        # Viterbi decode\n",
                "        best_tags = self.crf.decode(emissions, mask=mask)\n",
                "        \n",
                "        return best_tags"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "feature_extraction",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 10: Feature extraction for Bi-LSTM\n",
                "\n",
                "def extract_sentence_features(skipgram_model: Word2Vec, cnn_cache: Dict[str, np.ndarray],\n",
                "                               tokenized_sentence: List[str]) -> np.ndarray:\n",
                "    \"\"\"\n",
                "    Extract combined Skip-gram + CNN features for each character in a sentence.\n",
                "    \n",
                "    For each character, we use the embedding of its parent word.\n",
                "    Returns array of shape (num_chars, input_dim)\n",
                "    \"\"\"\n",
                "    features = []\n",
                "    \n",
                "    for word in tokenized_sentence:\n",
                "        # Get word-level embeddings\n",
                "        sg_emb = get_skipgram_embedding(skipgram_model, word)\n",
                "        cnn_emb = cnn_cache.get(word, np.zeros(CNN_OUTPUT_DIM))\n",
                "        \n",
                "        # Concatenate Skip-gram and CNN embeddings\n",
                "        word_emb = np.concatenate([sg_emb, cnn_emb])\n",
                "        \n",
                "        # Assign same embedding to each character in the word\n",
                "        for char in word:\n",
                "            if char in CHAR2ID and char != ' ':\n",
                "                features.append(word_emb)\n",
                "    \n",
                "    return np.array(features) if features else np.array([]).reshape(0, BILSTM_INPUT_DIM)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "collate_fn",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 11: Batch Collation Function\n",
                "\n",
                "def collate_batch(batch_data: List[Tuple[np.ndarray, List[int]]]):\n",
                "    \"\"\"\n",
                "    Collate function for DataLoader.\n",
                "    Pads sequences to the same length within a batch.\n",
                "    \n",
                "    Args:\n",
                "        batch_data: List of (features, tags) tuples\n",
                "    Returns:\n",
                "        features_padded: Tensor of shape (batch_size, max_seq_len, input_dim)\n",
                "        tags_padded: Tensor of shape (batch_size, max_seq_len)\n",
                "        mask: Tensor of shape (batch_size, max_seq_len)\n",
                "    \"\"\"\n",
                "    features_list = [item[0] for item in batch_data]\n",
                "    tags_list = [item[1] for item in batch_data]\n",
                "    \n",
                "    # Find max sequence length in batch\n",
                "    max_len = max(len(f) for f in features_list)\n",
                "    \n",
                "    batch_size = len(features_list)\n",
                "    \n",
                "    # Pad features and tags\n",
                "    features_padded = np.zeros((batch_size, max_len, BILSTM_INPUT_DIM))\n",
                "    tags_padded = np.zeros((batch_size, max_len), dtype=np.int64)\n",
                "    mask = np.zeros((batch_size, max_len), dtype=np.uint8)\n",
                "    \n",
                "    for i, (feat, tags) in enumerate(zip(features_list, tags_list)):\n",
                "        seq_len = len(feat)\n",
                "        features_padded[i, :seq_len, :] = feat\n",
                "        tags_padded[i, :seq_len] = tags\n",
                "        mask[i, :seq_len] = 1\n",
                "    \n",
                "    return (\n",
                "        torch.tensor(features_padded, dtype=torch.float32),\n",
                "        torch.tensor(tags_padded, dtype=torch.long),\n",
                "        torch.tensor(mask, dtype=torch.bool)\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "train_function",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 12: Train function\n",
                "\n",
                "def train(model: BiLSTMCRFModel, train_dataset, skipgram_model: Word2Vec,\n",
                "          cnn_cache: Dict[str, np.ndarray], model_path: str):\n",
                "    \"\"\"\n",
                "    Train the Bi-LSTM + CRF model.\n",
                "    \"\"\"\n",
                "    print(\"Preparing training data...\")\n",
                "    \n",
                "    # Extract features for all sentences\n",
                "    training_data = []\n",
                "    for idx in tqdm(range(len(train_dataset)), desc=\"Extracting features\"):\n",
                "        tokenized_sentence, diacritics = train_dataset[idx]\n",
                "        features = extract_sentence_features(skipgram_model, cnn_cache, tokenized_sentence)\n",
                "        \n",
                "        # Ensure features and diacritics have the same length\n",
                "        if len(features) == len(diacritics) and len(features) > 0:\n",
                "            training_data.append((features, diacritics))\n",
                "    \n",
                "    print(f\"Prepared {len(training_data)} valid training samples\")\n",
                "    \n",
                "    # Create DataLoader\n",
                "    train_loader = DataLoader(\n",
                "        training_data,\n",
                "        batch_size=BATCH_SIZE,\n",
                "        shuffle=True,\n",
                "        collate_fn=collate_batch\n",
                "    )\n",
                "    \n",
                "    # Setup optimizer\n",
                "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
                "    \n",
                "    model.to(DEVICE)\n",
                "    model.train()\n",
                "    \n",
                "    for epoch in range(NUM_EPOCHS):\n",
                "        total_loss = 0\n",
                "        num_batches = 0\n",
                "        \n",
                "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
                "        for features, tags, mask in progress_bar:\n",
                "            features = features.to(DEVICE)\n",
                "            tags = tags.to(DEVICE)\n",
                "            mask = mask.to(DEVICE)\n",
                "            \n",
                "            # Forward pass\n",
                "            loss = model(features, tags, mask)\n",
                "            \n",
                "            # Backward pass\n",
                "            optimizer.zero_grad()\n",
                "            loss.backward()\n",
                "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
                "            optimizer.step()\n",
                "            \n",
                "            total_loss += loss.item()\n",
                "            num_batches += 1\n",
                "            \n",
                "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
                "        \n",
                "        avg_loss = total_loss / num_batches\n",
                "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Average Loss: {avg_loss:.4f}\")\n",
                "    \n",
                "    # Save model\n",
                "    torch.save(model.state_dict(), model_path)\n",
                "    print(f\"Model saved to {model_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "evaluate_function",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 13: Evaluate function\n",
                "\n",
                "def evaluate(model: BiLSTMCRFModel, val_dataset, skipgram_model: Word2Vec,\n",
                "             cnn_cache: Dict[str, np.ndarray]):\n",
                "    \"\"\"\n",
                "    Evaluate the Bi-LSTM + CRF model on validation data.\n",
                "    \"\"\"\n",
                "    print(\"Preparing validation data...\")\n",
                "    \n",
                "    # Extract features for all sentences\n",
                "    val_data = []\n",
                "    last_char_info = []  # Track which positions are last char of word\n",
                "    \n",
                "    for idx in tqdm(range(len(val_dataset)), desc=\"Extracting validation features\"):\n",
                "        tokenized_sentence, diacritics = val_dataset[idx]\n",
                "        features = extract_sentence_features(skipgram_model, cnn_cache, tokenized_sentence)\n",
                "        \n",
                "        if len(features) == len(diacritics) and len(features) > 0:\n",
                "            val_data.append((features, diacritics))\n",
                "            \n",
                "            # Track last character positions\n",
                "            last_chars = []\n",
                "            for word in tokenized_sentence:\n",
                "                word_chars = [c for c in word if c in CHAR2ID and c != ' ']\n",
                "                for i, _ in enumerate(word_chars):\n",
                "                    last_chars.append(i == len(word_chars) - 1)\n",
                "            last_char_info.append(last_chars)\n",
                "    \n",
                "    print(f\"Prepared {len(val_data)} valid validation samples\")\n",
                "    \n",
                "    # Create DataLoader\n",
                "    val_loader = DataLoader(\n",
                "        val_data,\n",
                "        batch_size=BATCH_SIZE,\n",
                "        shuffle=False,\n",
                "        collate_fn=collate_batch\n",
                "    )\n",
                "    \n",
                "    model.to(DEVICE)\n",
                "    model.eval()\n",
                "    \n",
                "    all_predictions = []\n",
                "    all_targets = []\n",
                "    all_masks = []\n",
                "    \n",
                "    print(\"Running predictions...\")\n",
                "    with torch.no_grad():\n",
                "        for features, tags, mask in tqdm(val_loader, desc=\"Evaluating\"):\n",
                "            features = features.to(DEVICE)\n",
                "            mask = mask.to(DEVICE)\n",
                "            \n",
                "            # Decode using CRF\n",
                "            predictions = model.decode(features, mask)\n",
                "            \n",
                "            # Store results\n",
                "            all_predictions.extend(predictions)\n",
                "            all_targets.extend(tags.numpy().tolist())\n",
                "            all_masks.extend(mask.cpu().numpy().tolist())\n",
                "    \n",
                "    # Calculate accuracies\n",
                "    total_correct = 0\n",
                "    total_tokens = 0\n",
                "    total_correct_ending = 0\n",
                "    total_tokens_ending = 0\n",
                "    total_correct_without_ending = 0\n",
                "    total_tokens_without_ending = 0\n",
                "    \n",
                "    sample_idx = 0\n",
                "    for preds, targets, mask in zip(all_predictions, all_targets, all_masks):\n",
                "        last_chars = last_char_info[sample_idx] if sample_idx < len(last_char_info) else []\n",
                "        \n",
                "        for i, (pred, target, m) in enumerate(zip(preds, targets, mask)):\n",
                "            if m:  # Only count valid positions\n",
                "                total_tokens += 1\n",
                "                if pred == target:\n",
                "                    total_correct += 1\n",
                "                \n",
                "                is_last_char = last_chars[i] if i < len(last_chars) else False\n",
                "                if is_last_char:\n",
                "                    total_tokens_ending += 1\n",
                "                    if pred == target:\n",
                "                        total_correct_ending += 1\n",
                "                else:\n",
                "                    total_tokens_without_ending += 1\n",
                "                    if pred == target:\n",
                "                        total_correct_without_ending += 1\n",
                "        \n",
                "        sample_idx += 1\n",
                "    \n",
                "    val_accuracy = (total_correct / total_tokens) * 100 if total_tokens > 0 else 0\n",
                "    val_accuracy_ending = (total_correct_ending / total_tokens_ending) * 100 if total_tokens_ending > 0 else 0\n",
                "    val_accuracy_without_ending = (total_correct_without_ending / total_tokens_without_ending) * 100 if total_tokens_without_ending > 0 else 0\n",
                "    \n",
                "    print(f\"Validation Accuracy (Overall): {val_accuracy:.2f}%\")\n",
                "    print(f\"Validation Accuracy (Without Last Character): {val_accuracy_without_ending:.2f}%\")\n",
                "    print(f\"Validation Accuracy (Last Character): {val_accuracy_ending:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "predict_function",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 14: Predict function\n",
                "\n",
                "def predict(model: BiLSTMCRFModel, skipgram_model: Word2Vec,\n",
                "            cnn_cache: Dict[str, np.ndarray], sentence: str) -> List[int]:\n",
                "    \"\"\"\n",
                "    Predict diacritics for a single sentence.\n",
                "    \"\"\"\n",
                "    # Clean sentence\n",
                "    clean_sentence = sentence\n",
                "    for diacritic in DIACRITICS:\n",
                "        clean_sentence = clean_sentence.replace(diacritic, '')\n",
                "    \n",
                "    tokenized = clean_sentence.split()\n",
                "    \n",
                "    # Handle new words not in CNN cache\n",
                "    for word in tokenized:\n",
                "        if word not in cnn_cache:\n",
                "            cnn_cache[word] = np.zeros(CNN_OUTPUT_DIM)\n",
                "    \n",
                "    features = extract_sentence_features(skipgram_model, cnn_cache, tokenized)\n",
                "    \n",
                "    if len(features) == 0:\n",
                "        return []\n",
                "    \n",
                "    # Prepare input\n",
                "    features_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
                "    mask = torch.ones(1, len(features), dtype=torch.bool).to(DEVICE)\n",
                "    \n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        predictions = model.decode(features_tensor, mask)[0]\n",
                "    \n",
                "    return predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "infer_function",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 15: Infer function\n",
                "\n",
                "def infer(model: BiLSTMCRFModel, skipgram_model: Word2Vec,\n",
                "          cnn_cache: Dict[str, np.ndarray], input_path: str, output_path: str):\n",
                "    \"\"\"\n",
                "    Run inference on an input file and save results.\n",
                "    \"\"\"\n",
                "    with open(input_path, 'r', encoding='utf-8') as f:\n",
                "        input_data = f.readlines()\n",
                "    \n",
                "    output_list = []\n",
                "    output_csv = [[\"ID\", \"Label\"]]\n",
                "    current_id = 0\n",
                "    \n",
                "    for sentence in tqdm(input_data, desc=\"Inference\"):\n",
                "        sentence = sentence.strip()\n",
                "        if not sentence:\n",
                "            output_list.append(\"\")\n",
                "            continue\n",
                "        \n",
                "        clean_sentence = sentence\n",
                "        for diacritic in DIACRITICS:\n",
                "            clean_sentence = clean_sentence.replace(diacritic, '')\n",
                "        \n",
                "        predictions = predict(model, skipgram_model, cnn_cache, clean_sentence)\n",
                "        \n",
                "        diacritized_sentence = \"\"\n",
                "        pred_idx = 0\n",
                "        for char in clean_sentence:\n",
                "            diacritized_sentence += char\n",
                "            if char in ARABIC_LETTERS and pred_idx < len(predictions):\n",
                "                diacritic_id = predictions[pred_idx]\n",
                "                diacritic = ID2DIACRITIC.get(diacritic_id, '')\n",
                "                diacritized_sentence += diacritic\n",
                "                output_csv.append([current_id, diacritic_id])\n",
                "                current_id += 1\n",
                "                pred_idx += 1\n",
                "        \n",
                "        output_list.append(diacritized_sentence)\n",
                "    \n",
                "    with open(output_path, 'w', encoding='utf-8') as f:\n",
                "        for line in output_list:\n",
                "            f.write(line + '\\n')\n",
                "    \n",
                "    output_path_csv = os.path.splitext(output_path)[0] + \".csv\"\n",
                "    with open(output_path_csv, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
                "        writer = csv.writer(file)\n",
                "        writer.writerows(output_csv)\n",
                "    \n",
                "    print(f\"Output saved to {output_path} and {output_path_csv}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "load_train_data",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 186315 training sentences\n",
                        "Found 105795 unique words\n"
                    ]
                }
            ],
            "source": [
                "# Cell 16: Load training dataset\n",
                "train_dataset = generate_dataset(\"ArabicBiLSTMDataset\", \"../data/train.txt\")\n",
                "print(f\"Loaded {len(train_dataset)} training sentences\")\n",
                "print(f\"Found {len(train_dataset.unique_words)} unique words\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "load_skipgram",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training Skip-gram model...\n",
                        "Skip-gram model saved to ../models/SkipgramWordEmbeddings.model\n"
                    ]
                }
            ],
            "source": [
                "# Cell 17: Train or load Skip-gram model\n",
                "if os.path.exists(skipgram_model_path):\n",
                "    print(\"Loading existing Skip-gram model...\")\n",
                "    skipgram_model = load_skipgram_model(skipgram_model_path)\n",
                "else:\n",
                "    corpus = train_dataset.get_corpus_for_skipgram()\n",
                "    skipgram_model = train_skipgram_model(corpus, skipgram_model_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "init_cnn",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CNN Character Encoder initialized on cuda\n",
                        "CNN output dimension: 150\n"
                    ]
                }
            ],
            "source": [
                "# Cell 18: Initialize CNN Character Encoder\n",
                "cnn_model = CNNCharEncoder(\n",
                "    vocab_size=VOCAB_SIZE,\n",
                "    char_embedding_dim=CNN_CHAR_EMBEDDING_DIM,\n",
                "    num_filters=CNN_NUM_FILTERS,\n",
                "    kernel_sizes=CNN_KERNEL_SIZES\n",
                ").to(DEVICE)\n",
                "\n",
                "print(f\"CNN Character Encoder initialized on {DEVICE}\")\n",
                "print(f\"CNN output dimension: {cnn_model.output_dim}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "compute_cnn_cache",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Computing CNN embeddings for 105795 unique words on cuda...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "CNN batch processing: 100%|██████████| 207/207 [00:00<00:00, 451.26it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Computed 105795 CNN word embeddings\n",
                        "CNN cache saved to ../models/cnn_word_embeddings_cache.pkl\n",
                        "CNN model saved to ../models/CNNCharEncoder.pth\n"
                    ]
                }
            ],
            "source": [
                "# Cell 19: Pre-compute CNN embeddings for all unique words\n",
                "\n",
                "if os.path.exists(cnn_cache_path):\n",
                "    print(\"Loading existing CNN cache...\")\n",
                "    cnn_cache = load_cnn_cache(cnn_cache_path)\n",
                "else:\n",
                "    cnn_cache = compute_cnn_embeddings_batch(cnn_model, train_dataset.unique_words)\n",
                "    save_cnn_cache(cnn_cache, cnn_cache_path)\n",
                "\n",
                "# Save CNN model weights\n",
                "torch.save(cnn_model.state_dict(), cnn_model_path)\n",
                "print(f\"CNN model saved to {cnn_model_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "create_model",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Bi-LSTM + CRF Model created\n",
                        "Input dimension: 250\n",
                        "Hidden dimension: 256\n",
                        "Number of tags: 15\n",
                        "BiLSTMCRFModel(\n",
                        "  (input_projection): Linear(in_features=250, out_features=256, bias=True)\n",
                        "  (lstm): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
                        "  (hidden2tag): Linear(in_features=512, out_features=15, bias=True)\n",
                        "  (crf): CRF(num_tags=15)\n",
                        "  (dropout): Dropout(p=0.3, inplace=False)\n",
                        ")\n"
                    ]
                }
            ],
            "source": [
                "# Cell 20: Create Bi-LSTM + CRF model\n",
                "bilstm_crf_model = generate_model(\n",
                "    \"BiLSTMCRFModel\",\n",
                "    input_dim=BILSTM_INPUT_DIM,\n",
                "    hidden_dim=BILSTM_HIDDEN_DIM,\n",
                "    num_tags=NUM_TAGS,\n",
                "    num_layers=BILSTM_NUM_LAYERS,\n",
                "    dropout=BILSTM_DROPOUT\n",
                ")\n",
                "\n",
                "print(f\"Bi-LSTM + CRF Model created\")\n",
                "print(f\"Input dimension: {BILSTM_INPUT_DIM}\")\n",
                "print(f\"Hidden dimension: {BILSTM_HIDDEN_DIM}\")\n",
                "print(f\"Number of tags: {NUM_TAGS}\")\n",
                "print(bilstm_crf_model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "train_model",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Preparing training data...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Extracting features: 100%|██████████| 186315/186315 [00:04<00:00, 39388.45it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Prepared 20019 valid training samples\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/10: 100%|██████████| 626/626 [00:04<00:00, 128.68it/s, loss=1.4810]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/10, Average Loss: 2.7939\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 2/10: 100%|██████████| 626/626 [00:04<00:00, 143.43it/s, loss=1.5066]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 2/10, Average Loss: 1.6933\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 3/10: 100%|██████████| 626/626 [00:04<00:00, 147.37it/s, loss=1.3519]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 3/10, Average Loss: 1.4253\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 4/10: 100%|██████████| 626/626 [00:04<00:00, 146.42it/s, loss=0.5862]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 4/10, Average Loss: 1.2667\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 5/10: 100%|██████████| 626/626 [00:04<00:00, 143.03it/s, loss=1.4435]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 5/10, Average Loss: 1.1443\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 6/10: 100%|██████████| 626/626 [00:04<00:00, 143.13it/s, loss=0.7545]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 6/10, Average Loss: 1.0600\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 7/10: 100%|██████████| 626/626 [00:04<00:00, 145.30it/s, loss=0.2166]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 7/10, Average Loss: 0.9869\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 8/10: 100%|██████████| 626/626 [00:04<00:00, 138.62it/s, loss=0.4995]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 8/10, Average Loss: 0.9036\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 9/10: 100%|██████████| 626/626 [00:04<00:00, 145.76it/s, loss=0.1705]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 9/10, Average Loss: 0.8622\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 10/10: 100%|██████████| 626/626 [00:04<00:00, 141.55it/s, loss=0.5806]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 10/10, Average Loss: 0.8079\n",
                        "Model saved to ../models/BiLSTMCRFModel.pth\n"
                    ]
                }
            ],
            "source": [
                "# Cell 21: Train the model\n",
                "train(bilstm_crf_model, train_dataset, skipgram_model, cnn_cache, model_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "load_val_data",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Computing CNN embeddings for 2319 new validation words...\n",
                        "Computing CNN embeddings for 2319 unique words on cuda...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "CNN batch processing: 100%|██████████| 5/5 [00:00<00:00, 737.42it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Computed 2319 CNN word embeddings\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Cell 22: Load validation dataset and update CNN cache\n",
                "val_dataset = generate_dataset(\"ArabicBiLSTMDataset\", \"../data/val.txt\")\n",
                "\n",
                "# Add validation words to cache if not present\n",
                "new_words = [w for w in val_dataset.unique_words if w not in cnn_cache]\n",
                "if new_words:\n",
                "    print(f\"Computing CNN embeddings for {len(new_words)} new validation words...\")\n",
                "    new_cache = compute_cnn_embeddings_batch(cnn_model, new_words)\n",
                "    cnn_cache.update(new_cache)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "evaluate_model",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Preparing validation data...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Extracting validation features: 100%|██████████| 9068/9068 [00:00<00:00, 33428.53it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Prepared 964 valid validation samples\n",
                        "Running predictions...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Evaluating: 100%|██████████| 31/31 [00:00<00:00, 319.58it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Validation Accuracy (Overall): 90.12%\n",
                        "Validation Accuracy (Without Last Character): 90.39%\n",
                        "Validation Accuracy (Last Character): 89.21%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Cell 23: Evaluate the model\n",
                "evaluate(bilstm_crf_model, val_dataset, skipgram_model, cnn_cache)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "id": "inference",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Inference: 100%|██████████| 199/199 [00:02<00:00, 93.31it/s] "
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Output saved to ../output/output_bilstm_crf.txt and ../output/output_bilstm_crf.csv\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Cell 24: Run inference (optional)\n",
                "infer(bilstm_crf_model, skipgram_model, cnn_cache, input_path, output_path)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
