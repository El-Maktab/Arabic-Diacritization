{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "3f78b250",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1: Imports\n",
                "import numpy as np\n",
                "import re\n",
                "import os\n",
                "import csv\n",
                "import pickle\n",
                "from tqdm import tqdm\n",
                "from typing import List, Any, Dict, Tuple\n",
                "\n",
                "# PyTorch for CNN\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "# Skip-gram (Word2Vec)\n",
                "from gensim.models import Word2Vec\n",
                "\n",
                "# CRF\n",
                "import sklearn_crfsuite\n",
                "from sklearn_crfsuite import metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "1cddf29e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Paths configuration\n",
                "model_path = \"../models/CNNSkipgramCRFModel.pkl\"\n",
                "skipgram_model_path = \"../models/SkipgramWordEmbeddings.model\"\n",
                "cnn_model_path = \"../models/CNNCharEncoder.pth\"\n",
                "cnn_cache_path = \"../models/cnn_word_embeddings_cache.pkl\"\n",
                "input_path = \"../input/test_no_diacritics.txt\"\n",
                "output_path = \"../output/output_cnn_crf.txt\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "9c26ec0b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n"
                    ]
                }
            ],
            "source": [
                "# Cell 3: Constants and hyperparameters\n",
                "\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {DEVICE}\")\n",
                "\n",
                "# Global registries\n",
                "DATASET_REGISTRY: dict[str, Any] = {}\n",
                "MODEL_REGISTRY: dict[str, Any] = {}\n",
                "\n",
                "# Skip-gram hyperparameters\n",
                "SKIPGRAM_EMBEDDING_DIM = 100\n",
                "WINDOW_SIZE = 5\n",
                "MIN_COUNT = 1\n",
                "SG = 1  # 1 for Skip-gram, 0 for CBOW\n",
                "WORKERS = 4\n",
                "SKIPGRAM_EPOCHS = 10\n",
                "\n",
                "# CNN hyperparameters\n",
                "CNN_CHAR_EMBEDDING_DIM = 30\n",
                "CNN_NUM_FILTERS = 50\n",
                "CNN_KERNEL_SIZES = [2, 3, 4]  # n-gram sizes\n",
                "CNN_OUTPUT_DIM = CNN_NUM_FILTERS * len(CNN_KERNEL_SIZES)  # 150\n",
                "CNN_BATCH_SIZE = 512  # Batch size for CNN inference\n",
                "\n",
                "# CRF hyperparameters\n",
                "CRF_ALGORITHM = 'lbfgs'\n",
                "CRF_C1 = 0.1  # L1 regularization\n",
                "CRF_C2 = 0.1  # L2 regularization\n",
                "CRF_MAX_ITERATIONS = 100\n",
                "\n",
                "# Data parameters\n",
                "ARABIC_LETTERS = sorted(\n",
                "    np.load('../data/utils/arabic_letters.pkl', allow_pickle=True))\n",
                "DIACRITICS = sorted(np.load(\n",
                "    '../data/utils/diacritics.pkl', allow_pickle=True))\n",
                "PUNCTUATIONS = {\".\", \"،\", \":\", \"؛\", \"؟\", \"!\", '\"', \"-\"}\n",
                "\n",
                "VALID_CHARS = set(ARABIC_LETTERS).union(\n",
                "    set(DIACRITICS)).union(PUNCTUATIONS).union({\" \"})\n",
                "\n",
                "CHAR2ID = {char: id for id, char in enumerate(ARABIC_LETTERS)}\n",
                "CHAR2ID[\" \"] = len(ARABIC_LETTERS)\n",
                "CHAR2ID[\"<PAD>\"] = len(ARABIC_LETTERS) + 1\n",
                "PAD = CHAR2ID[\"<PAD>\"]\n",
                "SPACE = CHAR2ID[\" \"]\n",
                "ID2CHAR = {id: char for char, id in CHAR2ID.items()}\n",
                "VOCAB_SIZE = len(CHAR2ID)\n",
                "\n",
                "DIACRITIC2ID = np.load('../data/utils/diacritic2id.pkl', allow_pickle=True)\n",
                "ID2DIACRITIC = {id: diacritic for diacritic, id in DIACRITIC2ID.items()}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "7aa107dd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: Registry functions\n",
                "\n",
                "def register_dataset(name):\n",
                "    def decorator(cls):\n",
                "        DATASET_REGISTRY[name] = cls\n",
                "        return cls\n",
                "    return decorator\n",
                "\n",
                "\n",
                "def generate_dataset(dataset_name: str, *args, **kwargs):\n",
                "    try:\n",
                "        dataset_cls = DATASET_REGISTRY[dataset_name]\n",
                "    except KeyError:\n",
                "        raise ValueError(f\"Dataset '{dataset_name}' is not recognized.\")\n",
                "    return dataset_cls(*args, **kwargs)\n",
                "\n",
                "\n",
                "def register_model(name):\n",
                "    def decorator(cls):\n",
                "        MODEL_REGISTRY[name] = cls\n",
                "        return cls\n",
                "    return decorator\n",
                "\n",
                "\n",
                "def generate_model(model_name: str, *args, **kwargs):\n",
                "    try:\n",
                "        model_cls = MODEL_REGISTRY[model_name]\n",
                "    except KeyError:\n",
                "        raise ValueError(f\"Model '{model_name}' is not recognized.\")\n",
                "    return model_cls(*args, **kwargs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "85a77ff8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: Dataset class\n",
                "\n",
                "@register_dataset(\"ArabicCNNSkipgramDataset\")\n",
                "class ArabicCNNSkipgramDataset:\n",
                "    def __init__(self, file_path: str, skipgram_model: Word2Vec = None):\n",
                "        self.skipgram_model = skipgram_model\n",
                "        self.sentences_with_diacritics = self.load_data(file_path)\n",
                "        self.sentences_without_diacritics = self.extract_text_without_diacritics(\n",
                "            self.sentences_with_diacritics)\n",
                "        \n",
                "        # Tokenize into words for Skip-gram\n",
                "        self.tokenized_sentences = [sentence.split() for sentence in self.sentences_without_diacritics]\n",
                "        \n",
                "        # Extract diacritics per character\n",
                "        self.diacritics_per_sentence = [\n",
                "            self.extract_diacritics(sentence) \n",
                "            for sentence in self.sentences_with_diacritics\n",
                "        ]\n",
                "        \n",
                "        # Collect all unique words for CNN batch processing\n",
                "        self.unique_words = set()\n",
                "        for sentence in self.tokenized_sentences:\n",
                "            self.unique_words.update(sentence)\n",
                "        self.unique_words = list(self.unique_words)\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.sentences_without_diacritics)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        return self.tokenized_sentences[idx], self.diacritics_per_sentence[idx]\n",
                "\n",
                "    def load_data(self, file_path: str):\n",
                "        data = []\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            for line in f:\n",
                "                line = line.strip()\n",
                "                if line:\n",
                "                    line = re.sub(\n",
                "                        f'[^{re.escape(\"\".join(VALID_CHARS))}]', '', line)\n",
                "                    line = re.sub(r'\\s+', ' ', line)\n",
                "                    sentences = re.split(\n",
                "                        f'[{re.escape(\"\".join(PUNCTUATIONS))}]', line)\n",
                "                    sentences = [s.strip() for s in sentences if s.strip()]\n",
                "                    data.extend(sentences)\n",
                "        return np.array(data)\n",
                "\n",
                "    def extract_text_without_diacritics(self, dataY):\n",
                "        dataX = dataY.copy()\n",
                "        for diacritic, _ in DIACRITIC2ID.items():\n",
                "            dataX = np.char.replace(dataX, diacritic, '')\n",
                "        return dataX\n",
                "\n",
                "    def extract_diacritics(self, sentence: str):\n",
                "        \"\"\"Extract diacritics for each character in the sentence.\"\"\"\n",
                "        result = []\n",
                "        i = 0\n",
                "        n = len(sentence)\n",
                "        on_char = False\n",
                "\n",
                "        while i < n:\n",
                "            ch = sentence[i]\n",
                "            if ch in DIACRITICS:\n",
                "                on_char = False\n",
                "                if i+1 < n and sentence[i+1] in DIACRITICS:\n",
                "                    combined = ch + sentence[i+1]\n",
                "                    if combined in DIACRITIC2ID:\n",
                "                        result.append(str(DIACRITIC2ID[combined]))\n",
                "                        i += 2\n",
                "                        continue\n",
                "                result.append(str(DIACRITIC2ID[ch]))\n",
                "            elif ch in CHAR2ID:\n",
                "                if on_char:\n",
                "                    result.append(str(DIACRITIC2ID['']))\n",
                "                on_char = True\n",
                "            i += 1\n",
                "        if on_char:\n",
                "            result.append(str(DIACRITIC2ID['']))\n",
                "        return result\n",
                "\n",
                "    def get_corpus_for_skipgram(self):\n",
                "        \"\"\"Return tokenized sentences for Skip-gram training.\"\"\"\n",
                "        return self.tokenized_sentences"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "6270c87e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: CNN Character Encoder\n",
                "\n",
                "class CNNCharEncoder(nn.Module):\n",
                "    \"\"\"\n",
                "    CNN-based character encoder that produces a fixed-size embedding for each word\n",
                "    based on its character sequence.\n",
                "    \"\"\"\n",
                "    def __init__(self, vocab_size, char_embedding_dim, num_filters, kernel_sizes):\n",
                "        super(CNNCharEncoder, self).__init__()\n",
                "        \n",
                "        self.char_embedding = nn.Embedding(vocab_size, char_embedding_dim, padding_idx=PAD)\n",
                "        \n",
                "        # Multiple CNN layers with different kernel sizes to capture different n-grams\n",
                "        self.convs = nn.ModuleList([\n",
                "            nn.Conv1d(in_channels=char_embedding_dim, \n",
                "                     out_channels=num_filters, \n",
                "                     kernel_size=k,\n",
                "                     padding=k//2)\n",
                "            for k in kernel_sizes\n",
                "        ])\n",
                "        \n",
                "        self.output_dim = num_filters * len(kernel_sizes)\n",
                "    \n",
                "    def forward(self, char_ids):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            char_ids: Tensor of shape (batch_size, max_word_len)\n",
                "        Returns:\n",
                "            word_embedding: Tensor of shape (batch_size, output_dim)\n",
                "        \"\"\"\n",
                "        # Embed characters: (batch, max_word_len, char_embedding_dim)\n",
                "        embedded = self.char_embedding(char_ids)\n",
                "        \n",
                "        # Transpose for Conv1d: (batch, char_embedding_dim, max_word_len)\n",
                "        embedded = embedded.transpose(1, 2)\n",
                "        \n",
                "        # Apply each conv layer and max-pool\n",
                "        conv_outputs = []\n",
                "        for conv in self.convs:\n",
                "            conv_out = F.relu(conv(embedded))  # (batch, num_filters, seq_len)\n",
                "            pooled = F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2)  # (batch, num_filters)\n",
                "            conv_outputs.append(pooled)\n",
                "        \n",
                "        # Concatenate all conv outputs\n",
                "        word_embedding = torch.cat(conv_outputs, dim=1)  # (batch, output_dim)\n",
                "        return word_embedding\n",
                "\n",
                "\n",
                "def word_to_char_ids(word: str, max_len: int = 20) -> List[int]:\n",
                "    \"\"\"Convert a word to a list of character IDs, padded to max_len.\"\"\"\n",
                "    char_ids = [CHAR2ID.get(c, PAD) for c in word if c in CHAR2ID]\n",
                "    # Pad or truncate\n",
                "    if len(char_ids) < max_len:\n",
                "        char_ids = char_ids + [PAD] * (max_len - len(char_ids))\n",
                "    else:\n",
                "        char_ids = char_ids[:max_len]\n",
                "    return char_ids"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "batch_cnn",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7: OPTIMIZED - Batch CNN embedding computation with caching\n",
                "\n",
                "def compute_cnn_embeddings_batch(cnn_model: CNNCharEncoder, words: List[str], \n",
                "                                  batch_size: int = CNN_BATCH_SIZE) -> Dict[str, np.ndarray]:\n",
                "    \"\"\"\n",
                "    Compute CNN embeddings for all words in batch on GPU.\n",
                "    Returns a dictionary mapping word -> embedding.\n",
                "    \"\"\"\n",
                "    print(f\"Computing CNN embeddings for {len(words)} unique words on {DEVICE}...\")\n",
                "    cnn_model.eval()\n",
                "    cnn_model.to(DEVICE)\n",
                "    \n",
                "    word_to_embedding = {}\n",
                "    \n",
                "    # Process in batches\n",
                "    for i in tqdm(range(0, len(words), batch_size), desc=\"CNN batch processing\"):\n",
                "        batch_words = words[i:i+batch_size]\n",
                "        \n",
                "        # Convert words to char IDs\n",
                "        batch_char_ids = [word_to_char_ids(word) for word in batch_words]\n",
                "        batch_tensor = torch.tensor(batch_char_ids, dtype=torch.long).to(DEVICE)\n",
                "        \n",
                "        # Forward pass on GPU\n",
                "        with torch.no_grad():\n",
                "            embeddings = cnn_model(batch_tensor).cpu().numpy()\n",
                "        \n",
                "        # Store in cache\n",
                "        for word, emb in zip(batch_words, embeddings):\n",
                "            word_to_embedding[word] = emb\n",
                "    \n",
                "    print(f\"Computed {len(word_to_embedding)} CNN word embeddings\")\n",
                "    return word_to_embedding\n",
                "\n",
                "\n",
                "def save_cnn_cache(cache: Dict[str, np.ndarray], path: str):\n",
                "    \"\"\"Save CNN embedding cache to disk.\"\"\"\n",
                "    with open(path, 'wb') as f:\n",
                "        pickle.dump(cache, f)\n",
                "    print(f\"CNN cache saved to {path}\")\n",
                "\n",
                "\n",
                "def load_cnn_cache(path: str) -> Dict[str, np.ndarray]:\n",
                "    \"\"\"Load CNN embedding cache from disk.\"\"\"\n",
                "    with open(path, 'rb') as f:\n",
                "        cache = pickle.load(f)\n",
                "    print(f\"Loaded CNN cache with {len(cache)} words\")\n",
                "    return cache"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "cnn_skipgram",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 8: Skip-gram training and OPTIMIZED feature extraction\n",
                "\n",
                "def train_skipgram_model(corpus: List[List[str]], save_path: str) -> Word2Vec:\n",
                "    \"\"\"Train a Skip-gram Word2Vec model on the corpus.\"\"\"\n",
                "    print(\"Training Skip-gram model...\")\n",
                "    model = Word2Vec(\n",
                "        sentences=corpus,\n",
                "        vector_size=SKIPGRAM_EMBEDDING_DIM,\n",
                "        window=WINDOW_SIZE,\n",
                "        min_count=MIN_COUNT,\n",
                "        sg=SG,\n",
                "        workers=WORKERS,\n",
                "        epochs=SKIPGRAM_EPOCHS\n",
                "    )\n",
                "    model.save(save_path)\n",
                "    print(f\"Skip-gram model saved to {save_path}\")\n",
                "    return model\n",
                "\n",
                "\n",
                "def load_skipgram_model(path: str) -> Word2Vec:\n",
                "    \"\"\"Load a pre-trained Skip-gram model.\"\"\"\n",
                "    return Word2Vec.load(path)\n",
                "\n",
                "\n",
                "# Pre-cache Skip-gram lookups for speed\n",
                "SKIPGRAM_CACHE: Dict[str, np.ndarray] = {}\n",
                "\n",
                "def get_skipgram_embedding(skipgram_model: Word2Vec, word: str) -> np.ndarray:\n",
                "    \"\"\"Get Skip-gram embedding with caching.\"\"\"\n",
                "    if word not in SKIPGRAM_CACHE:\n",
                "        if word in skipgram_model.wv:\n",
                "            SKIPGRAM_CACHE[word] = skipgram_model.wv[word]\n",
                "        else:\n",
                "            SKIPGRAM_CACHE[word] = np.zeros(SKIPGRAM_EMBEDDING_DIM)\n",
                "    return SKIPGRAM_CACHE[word]\n",
                "\n",
                "\n",
                "def word_to_features_cached(skipgram_model: Word2Vec, cnn_cache: Dict[str, np.ndarray],\n",
                "                            sentence: List[str], char_idx: int, word_idx: int, \n",
                "                            char_in_word: str, is_last_char: bool) -> Dict[str, Any]:\n",
                "    \"\"\"\n",
                "    Extract features for a single character using CACHED embeddings.\n",
                "    This is much faster than computing CNN embeddings on the fly.\n",
                "    \"\"\"\n",
                "    features = {\n",
                "        'bias': 1.0,\n",
                "        'char': char_in_word,\n",
                "        'char_idx_in_sentence': char_idx,\n",
                "        'is_last_char_in_word': is_last_char,\n",
                "    }\n",
                "    \n",
                "    current_word = sentence[word_idx]\n",
                "    \n",
                "    # === Skip-gram word embedding (cached) ===\n",
                "    sg_emb = get_skipgram_embedding(skipgram_model, current_word)\n",
                "    for i, val in enumerate(sg_emb):\n",
                "        features[f'sg_word_emb_{i}'] = float(val)\n",
                "    \n",
                "    # === CNN character embedding (from pre-computed cache) ===\n",
                "    cnn_emb = cnn_cache.get(current_word, np.zeros(CNN_OUTPUT_DIM))\n",
                "    for i, val in enumerate(cnn_emb):\n",
                "        features[f'cnn_word_emb_{i}'] = float(val)\n",
                "    \n",
                "    # === Previous word Skip-gram embedding (context) ===\n",
                "    if word_idx > 0:\n",
                "        prev_emb = get_skipgram_embedding(skipgram_model, sentence[word_idx - 1])\n",
                "        for i, val in enumerate(prev_emb):\n",
                "            features[f'prev_sg_emb_{i}'] = float(val)\n",
                "    else:\n",
                "        features['BOS'] = True\n",
                "        for i in range(SKIPGRAM_EMBEDDING_DIM):\n",
                "            features[f'prev_sg_emb_{i}'] = 0.0\n",
                "    \n",
                "    # === Next word Skip-gram embedding (context) ===\n",
                "    if word_idx < len(sentence) - 1:\n",
                "        next_emb = get_skipgram_embedding(skipgram_model, sentence[word_idx + 1])\n",
                "        for i, val in enumerate(next_emb):\n",
                "            features[f'next_sg_emb_{i}'] = float(val)\n",
                "    else:\n",
                "        features['EOS'] = True\n",
                "        for i in range(SKIPGRAM_EMBEDDING_DIM):\n",
                "            features[f'next_sg_emb_{i}'] = 0.0\n",
                "    \n",
                "    return features\n",
                "\n",
                "\n",
                "def sentence_to_features_cached(skipgram_model: Word2Vec, cnn_cache: Dict[str, np.ndarray],\n",
                "                                tokenized_sentence: List[str], \n",
                "                                sentence_without_diacritics: str) -> List[Dict[str, Any]]:\n",
                "    \"\"\"\n",
                "    Convert an entire sentence to CRF features using cached embeddings.\n",
                "    \"\"\"\n",
                "    features = []\n",
                "    char_idx = 0\n",
                "    \n",
                "    for word_idx, word in enumerate(tokenized_sentence):\n",
                "        for i, char in enumerate(word):\n",
                "            if char in CHAR2ID and char != ' ':\n",
                "                is_last_char = (i == len(word) - 1)\n",
                "                feat = word_to_features_cached(\n",
                "                    skipgram_model, cnn_cache, tokenized_sentence, \n",
                "                    char_idx, word_idx, char, is_last_char\n",
                "                )\n",
                "                features.append(feat)\n",
                "                char_idx += 1\n",
                "    \n",
                "    return features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "crf_model",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 9: CRF Model class\n",
                "\n",
                "@register_model(\"CNNSkipgramCRFModel\")\n",
                "class CNNSkipgramCRFModel:\n",
                "    def __init__(self):\n",
                "        self.crf = sklearn_crfsuite.CRF(\n",
                "            algorithm=CRF_ALGORITHM,\n",
                "            c1=CRF_C1,\n",
                "            c2=CRF_C2,\n",
                "            max_iterations=CRF_MAX_ITERATIONS,\n",
                "            all_possible_transitions=True\n",
                "        )\n",
                "    \n",
                "    def fit(self, X_train: List[List[Dict]], y_train: List[List[str]]):\n",
                "        \"\"\"Train the CRF model.\"\"\"\n",
                "        self.crf.fit(X_train, y_train)\n",
                "    \n",
                "    def predict(self, X: List[List[Dict]]) -> List[List[str]]:\n",
                "        \"\"\"Predict diacritics for input sequences.\"\"\"\n",
                "        return self.crf.predict(X)\n",
                "    \n",
                "    def save(self, path: str):\n",
                "        \"\"\"Save the CRF model to disk.\"\"\"\n",
                "        with open(path, 'wb') as f:\n",
                "            pickle.dump(self.crf, f)\n",
                "    \n",
                "    def load(self, path: str):\n",
                "        \"\"\"Load a CRF model from disk.\"\"\"\n",
                "        with open(path, 'rb') as f:\n",
                "            self.crf = pickle.load(f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "a782bf2f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 10: OPTIMIZED Train function using cached embeddings\n",
                "\n",
                "def train(model: CNNSkipgramCRFModel, train_dataset: ArabicCNNSkipgramDataset,\n",
                "          skipgram_model: Word2Vec, cnn_cache: Dict[str, np.ndarray], model_path: str):\n",
                "    \"\"\"\n",
                "    Train the CRF model using pre-computed CNN + Skip-gram features.\n",
                "    \"\"\"\n",
                "    print(\"Preparing training data for CRF...\")\n",
                "    X_train = []\n",
                "    y_train = []\n",
                "    \n",
                "    for idx in tqdm(range(len(train_dataset)), desc=\"Extracting features\"):\n",
                "        tokenized_sentence, diacritics = train_dataset[idx]\n",
                "        sentence_without_diacritics = train_dataset.sentences_without_diacritics[idx]\n",
                "        \n",
                "        features = sentence_to_features_cached(\n",
                "            skipgram_model, cnn_cache, tokenized_sentence, sentence_without_diacritics\n",
                "        )\n",
                "        \n",
                "        # Ensure features and diacritics have the same length\n",
                "        if len(features) == len(diacritics):\n",
                "            X_train.append(features)\n",
                "            y_train.append(diacritics)\n",
                "    \n",
                "    print(f\"Training CRF on {len(X_train)} sentences...\")\n",
                "    model.fit(X_train, y_train)\n",
                "    model.save(model_path)\n",
                "    print(f\"CRF model saved to {model_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "f00cc728",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 11: OPTIMIZED Evaluate function\n",
                "\n",
                "def evaluate(model: CNNSkipgramCRFModel, val_dataset: ArabicCNNSkipgramDataset,\n",
                "             skipgram_model: Word2Vec, cnn_cache: Dict[str, np.ndarray]):\n",
                "    \"\"\"\n",
                "    Evaluate the CRF model on the validation dataset.\n",
                "    \"\"\"\n",
                "    print(\"Preparing validation data...\")\n",
                "    X_val = []\n",
                "    y_val = []\n",
                "    last_char_indices = []\n",
                "    \n",
                "    for idx in tqdm(range(len(val_dataset)), desc=\"Extracting validation features\"):\n",
                "        tokenized_sentence, diacritics = val_dataset[idx]\n",
                "        sentence_without_diacritics = val_dataset.sentences_without_diacritics[idx]\n",
                "        \n",
                "        features = sentence_to_features_cached(\n",
                "            skipgram_model, cnn_cache, tokenized_sentence, sentence_without_diacritics\n",
                "        )\n",
                "        \n",
                "        if len(features) == len(diacritics):\n",
                "            X_val.append(features)\n",
                "            y_val.append(diacritics)\n",
                "            sentence_last_chars = [feat.get('is_last_char_in_word', False) for feat in features]\n",
                "            last_char_indices.append(sentence_last_chars)\n",
                "    \n",
                "    print(\"Running predictions...\")\n",
                "    y_pred = model.predict(X_val)\n",
                "    \n",
                "    # Calculate accuracies\n",
                "    total_correct = 0\n",
                "    total_tokens = 0\n",
                "    total_correct_ending = 0\n",
                "    total_tokens_ending = 0\n",
                "    total_correct_without_ending = 0\n",
                "    total_tokens_without_ending = 0\n",
                "    \n",
                "    for sent_idx in range(len(y_val)):\n",
                "        for token_idx in range(len(y_val[sent_idx])):\n",
                "            pred = y_pred[sent_idx][token_idx]\n",
                "            true = y_val[sent_idx][token_idx]\n",
                "            is_last_char = last_char_indices[sent_idx][token_idx]\n",
                "            \n",
                "            total_tokens += 1\n",
                "            if pred == true:\n",
                "                total_correct += 1\n",
                "            \n",
                "            if is_last_char:\n",
                "                total_tokens_ending += 1\n",
                "                if pred == true:\n",
                "                    total_correct_ending += 1\n",
                "            else:\n",
                "                total_tokens_without_ending += 1\n",
                "                if pred == true:\n",
                "                    total_correct_without_ending += 1\n",
                "    \n",
                "    val_accuracy = (total_correct / total_tokens) * 100 if total_tokens > 0 else 0\n",
                "    val_accuracy_ending = (total_correct_ending / total_tokens_ending) * 100 if total_tokens_ending > 0 else 0\n",
                "    val_accuracy_without_ending = (total_correct_without_ending / total_tokens_without_ending) * 100 if total_tokens_without_ending > 0 else 0\n",
                "    \n",
                "    print(f\"Validation Accuracy (Overall): {val_accuracy:.2f}%\")\n",
                "    print(f\"Validation Accuracy (Without Last Character): {val_accuracy_without_ending:.2f}%\")\n",
                "    print(f\"Validation Accuracy (Last Character): {val_accuracy_ending:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "3ed91e8d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 12: Predict function\n",
                "\n",
                "def predict(model: CNNSkipgramCRFModel, skipgram_model: Word2Vec,\n",
                "            cnn_cache: Dict[str, np.ndarray], sentence: str) -> List[str]:\n",
                "    \"\"\"\n",
                "    Predict diacritics for a single sentence.\n",
                "    \"\"\"\n",
                "    clean_sentence = sentence\n",
                "    for diacritic in DIACRITICS:\n",
                "        clean_sentence = clean_sentence.replace(diacritic, '')\n",
                "    \n",
                "    tokenized = clean_sentence.split()\n",
                "    features = sentence_to_features_cached(skipgram_model, cnn_cache, tokenized, clean_sentence)\n",
                "    \n",
                "    if not features:\n",
                "        return []\n",
                "    \n",
                "    predictions = model.predict([features])[0]\n",
                "    return predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "02a98acc",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 13: Infer function\n",
                "\n",
                "def infer(model: CNNSkipgramCRFModel, skipgram_model: Word2Vec,\n",
                "          cnn_cache: Dict[str, np.ndarray], input_path: str, output_path: str):\n",
                "    \"\"\"\n",
                "    Run inference on an input file and save results.\n",
                "    \"\"\"\n",
                "    with open(input_path, 'r', encoding='utf-8') as f:\n",
                "        input_data = f.readlines()\n",
                "    \n",
                "    output_list = []\n",
                "    output_csv = [[\"ID\", \"Label\"]]\n",
                "    current_id = 0\n",
                "    \n",
                "    for sentence in tqdm(input_data, desc=\"Inference\"):\n",
                "        sentence = sentence.strip()\n",
                "        if not sentence:\n",
                "            output_list.append(\"\")\n",
                "            continue\n",
                "        \n",
                "        clean_sentence = sentence\n",
                "        for diacritic in DIACRITICS:\n",
                "            clean_sentence = clean_sentence.replace(diacritic, '')\n",
                "        \n",
                "        predictions = predict(model, skipgram_model, cnn_cache, clean_sentence)\n",
                "        \n",
                "        diacritized_sentence = \"\"\n",
                "        pred_idx = 0\n",
                "        for char in clean_sentence:\n",
                "            diacritized_sentence += char\n",
                "            if char in ARABIC_LETTERS and pred_idx < len(predictions):\n",
                "                diacritic_id = int(predictions[pred_idx])\n",
                "                diacritic = ID2DIACRITIC.get(diacritic_id, '')\n",
                "                diacritized_sentence += diacritic\n",
                "                output_csv.append([current_id, diacritic_id])\n",
                "                current_id += 1\n",
                "                pred_idx += 1\n",
                "        \n",
                "        output_list.append(diacritized_sentence)\n",
                "    \n",
                "    with open(output_path, 'w', encoding='utf-8') as f:\n",
                "        for line in output_list:\n",
                "            f.write(line + '\\n')\n",
                "    \n",
                "    output_path_csv = os.path.splitext(output_path)[0] + \".csv\"\n",
                "    with open(output_path_csv, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
                "        writer = csv.writer(file)\n",
                "        writer.writerows(output_csv)\n",
                "    \n",
                "    print(f\"Output saved to {output_path} and {output_path_csv}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "03fc9994",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 186315 training sentences\n",
                        "Found 105795 unique words\n"
                    ]
                }
            ],
            "source": [
                "# Cell 14: Load training dataset\n",
                "train_dataset = generate_dataset(\"ArabicCNNSkipgramDataset\", \"../data/train.txt\")\n",
                "print(f\"Loaded {len(train_dataset)} training sentences\")\n",
                "print(f\"Found {len(train_dataset.unique_words)} unique words\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "8eba76fb",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading existing Skip-gram model...\n"
                    ]
                }
            ],
            "source": [
                "# Cell 15: Train or load Skip-gram model\n",
                "if os.path.exists(skipgram_model_path):\n",
                "    print(\"Loading existing Skip-gram model...\")\n",
                "    skipgram_model = load_skipgram_model(skipgram_model_path)\n",
                "else:\n",
                "    corpus = train_dataset.get_corpus_for_skipgram()\n",
                "    skipgram_model = train_skipgram_model(corpus, skipgram_model_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "cnn_init",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CNN Character Encoder initialized on cuda\n",
                        "CNN output dimension: 150\n"
                    ]
                }
            ],
            "source": [
                "# Cell 16: Initialize CNN Character Encoder on GPU\n",
                "cnn_model = CNNCharEncoder(\n",
                "    vocab_size=VOCAB_SIZE,\n",
                "    char_embedding_dim=CNN_CHAR_EMBEDDING_DIM,\n",
                "    num_filters=CNN_NUM_FILTERS,\n",
                "    kernel_sizes=CNN_KERNEL_SIZES\n",
                ").to(DEVICE)\n",
                "\n",
                "print(f\"CNN Character Encoder initialized on {DEVICE}\")\n",
                "print(f\"CNN output dimension: {cnn_model.output_dim}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "compute_cache",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading existing CNN cache...\n",
                        "Loaded CNN cache with 105795 words\n",
                        "CNN model saved to ../models/CNNCharEncoder.pth\n"
                    ]
                }
            ],
            "source": [
                "# Cell 17: PRE-COMPUTE CNN embeddings for all unique words (GPU batched)\n",
                "# This is the key optimization - compute once, use many times\n",
                "\n",
                "if os.path.exists(cnn_cache_path):\n",
                "    print(\"Loading existing CNN cache...\")\n",
                "    cnn_cache = load_cnn_cache(cnn_cache_path)\n",
                "else:\n",
                "    cnn_cache = compute_cnn_embeddings_batch(cnn_model, train_dataset.unique_words)\n",
                "    save_cnn_cache(cnn_cache, cnn_cache_path)\n",
                "\n",
                "# Save CNN model weights\n",
                "torch.save(cnn_model.state_dict(), cnn_model_path)\n",
                "print(f\"CNN model saved to {cnn_model_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "ef3213c5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 18: Create CRF model\n",
                "crf_model = generate_model(\"CNNSkipgramCRFModel\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "94e5422c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Preparing training data for CRF...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Extracting features: 100%|██████████| 186315/186315 [07:55<00:00, 392.01it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training CRF on 20019 sentences...\n",
                        "CRF model saved to ../models/CNNSkipgramCRFModel.pkl\n"
                    ]
                }
            ],
            "source": [
                "# Cell 19: Train CRF model with CACHED features (FAST!)\n",
                "train(crf_model, train_dataset, skipgram_model, cnn_cache, model_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "2daacada",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Computing CNN embeddings for 2319 new validation words...\n",
                        "Computing CNN embeddings for 2319 unique words on cuda...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "CNN batch processing: 100%|██████████| 5/5 [00:00<00:00, 24.03it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Computed 2319 CNN word embeddings\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Cell 20: Load validation dataset and add its words to cache\n",
                "val_dataset = generate_dataset(\"ArabicCNNSkipgramDataset\", \"../data/val.txt\")\n",
                "\n",
                "# Add validation words to cache if not present\n",
                "new_words = [w for w in val_dataset.unique_words if w not in cnn_cache]\n",
                "if new_words:\n",
                "    print(f\"Computing CNN embeddings for {len(new_words)} new validation words...\")\n",
                "    new_cache = compute_cnn_embeddings_batch(cnn_model, new_words)\n",
                "    cnn_cache.update(new_cache)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "df45c57a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Preparing validation data...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Extracting validation features: 100%|██████████| 9068/9068 [00:22<00:00, 398.16it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running predictions...\n",
                        "Validation Accuracy (Overall): 83.67%\n",
                        "Validation Accuracy (Without Last Character): 83.21%\n",
                        "Validation Accuracy (Last Character): 85.17%\n"
                    ]
                }
            ],
            "source": [
                "# Cell 21: Evaluate the model\n",
                "evaluate(crf_model, val_dataset, skipgram_model, cnn_cache)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "ba7d24bf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 22: Run inference (optional)\n",
                "# infer(crf_model, skipgram_model, cnn_cache, input_path, output_path)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
