{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f78b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cddf29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"../models/ArabicBiLSTMModel.pth\"\n",
    "input_path = f\"../input/dataset_no_diacritics.txt\"\n",
    "output_path = f\"../output/output.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c26ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configurations\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Global registries\n",
    "DATASET_REGISTRY: dict[str, Any] = {}\n",
    "MODEL_REGISTRY: dict[str, Any] = {}\n",
    "\n",
    "# Model hyperparameters\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_LAYERS = 3\n",
    "DROPOUT = 0.2\n",
    "\n",
    "# Data parameters\n",
    "ARABIC_LETTERS = sorted(\n",
    "    np.load('../data/utils/arabic_letters.pkl', allow_pickle=True))\n",
    "DIACRITICS = sorted(np.load(\n",
    "    '../data/utils/diacritics.pkl', allow_pickle=True))\n",
    "PUNCTUATIONS = {\".\", \"،\", \":\", \"؛\", \"؟\", \"!\", '\"', \"-\"}\n",
    "\n",
    "VALID_CHARS = set(ARABIC_LETTERS).union(\n",
    "    set(DIACRITICS)).union(PUNCTUATIONS).union({\" \"})\n",
    "\n",
    "CHAR2ID = {char: id for id, char in enumerate(ARABIC_LETTERS)}\n",
    "CHAR2ID[\" \"] = len(ARABIC_LETTERS)\n",
    "CHAR2ID[\"<PAD>\"] = len(ARABIC_LETTERS) + 1\n",
    "PAD = CHAR2ID[\"<PAD>\"]\n",
    "SPACE = CHAR2ID[\" \"]\n",
    "ID2CHAR = {id: char for char, id in CHAR2ID.items()}\n",
    "\n",
    "DIACRITIC2ID = np.load('../data/utils/diacritic2id.pkl', allow_pickle=True)\n",
    "ID2DIACRITIC = {id: diacritic for diacritic, id in DIACRITIC2ID.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aa107dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def register_dataset(name):\n",
    "    def decorator(cls):\n",
    "        DATASET_REGISTRY[name] = cls\n",
    "        return cls\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def generate_dataset(dataset_name: str, *args, **kwargs):\n",
    "    try:\n",
    "        dataset_cls = DATASET_REGISTRY[dataset_name]\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Dataset '{dataset_name}' is not recognized.\")\n",
    "    return dataset_cls(*args, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def register_model(name):\n",
    "    def decorator(cls):\n",
    "        MODEL_REGISTRY[name] = cls\n",
    "        return cls\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def generate_model(model_name: str, *args, **kwargs):\n",
    "    try:\n",
    "        model_cls = MODEL_REGISTRY[model_name]\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Model '{model_name}' is not recognized.\")\n",
    "    return model_cls(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85a77ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@register_dataset(\"ArabicDataset\")\n",
    "class ArabicDataset(Dataset):\n",
    "    def __init__(self, file_path: str):\n",
    "        self.data_X, self.data_Y = self.generate_tensor_data(file_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_X[idx], self.data_Y[idx]\n",
    "\n",
    "    def generate_tensor_data(self, data_path: str):\n",
    "        data_Y = self.load_data(data_path)\n",
    "        data_X = self.extract_text_without_diacritics(data_Y)\n",
    "\n",
    "        encoded_data_X, encoded_data_Y = self.encode_data(data_X, data_Y)\n",
    "        data_X = torch.tensor(\n",
    "            encoded_data_X, dtype=torch.int64)\n",
    "        data_Y = torch.tensor(\n",
    "            encoded_data_Y, dtype=torch.int64)\n",
    "\n",
    "        return data_X, data_Y\n",
    "\n",
    "    def load_data(self, file_path: str):\n",
    "        data = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    line = re.sub(\n",
    "                        f'[^{re.escape(\"\".join(VALID_CHARS))}]', '', line)\n",
    "                    line = re.sub(r'\\s+', ' ', line)\n",
    "                    sentences = re.split(\n",
    "                        f'[{re.escape(\"\".join(PUNCTUATIONS))}]', line)\n",
    "                    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "                    data.extend(sentences)\n",
    "\n",
    "        return np.array(data)\n",
    "\n",
    "    def extract_text_without_diacritics(self, dataY):\n",
    "        dataX = dataY.copy()\n",
    "        for diacritic, _ in DIACRITIC2ID.items():\n",
    "            dataX = np.char.replace(\n",
    "                dataX, diacritic, '')\n",
    "        return dataX\n",
    "\n",
    "    def encode_data(self, dataX: List[str], dataY: List[str]):\n",
    "        encoded_data_X = []\n",
    "        for sentence in dataX:\n",
    "            encoded_data_X.append([CHAR2ID[char]\n",
    "                                   for char in sentence if char in CHAR2ID])\n",
    "        encoded_data_Y = []\n",
    "        for sentence in dataY:\n",
    "            encoded_data_Y.append(self.extract_diacritics(sentence))\n",
    "\n",
    "        max_sentence_len = max(len(sentence) for sentence in encoded_data_X)\n",
    "        padded_dataX = np.full(\n",
    "            (len(encoded_data_X), max_sentence_len), PAD, dtype=np.int64)\n",
    "        for i, seq in enumerate(encoded_data_X):\n",
    "            padded_dataX[i, :len(seq)] = seq\n",
    "\n",
    "        padded_dataY = np.full(\n",
    "            (len(encoded_data_Y), max_sentence_len), PAD, dtype=np.int64)\n",
    "        for i, seq in enumerate(encoded_data_Y):\n",
    "            padded_dataY[i, :len(seq)] = seq\n",
    "\n",
    "        return padded_dataX, padded_dataY\n",
    "\n",
    "    def extract_diacritics(self, sentence: str):\n",
    "        result = []\n",
    "        i = 0\n",
    "        n = len(sentence)\n",
    "        on_char = False\n",
    "\n",
    "        while i < n:\n",
    "            ch = sentence[i]\n",
    "            if ch in DIACRITICS:\n",
    "                on_char = False\n",
    "                # check if next char forms a stacked diacritic\n",
    "                if i+1 < n and sentence[i+1] in DIACRITICS:\n",
    "                    combined = ch + sentence[i+1]\n",
    "                    if combined in DIACRITIC2ID:\n",
    "                        result.append(DIACRITIC2ID[combined])\n",
    "                        i += 2\n",
    "                        continue\n",
    "                result.append(DIACRITIC2ID[ch])\n",
    "            elif ch in CHAR2ID:\n",
    "                if on_char:\n",
    "                    result.append(DIACRITIC2ID[''])\n",
    "                on_char = True\n",
    "            i += 1\n",
    "        if on_char:\n",
    "            result.append(DIACRITIC2ID[''])\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6270c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@register_model(\"LSTMArabicModel\")\n",
    "class LSTMArabicModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, PAD):\n",
    "        super(LSTMArabicModel, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embedding_dim, padding_idx=PAD)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
    "                            batch_first=True, bidirectional=True,\n",
    "                            num_layers=NUM_LAYERS, dropout=DROPOUT)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        output = self.fc(lstm_out)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a782bf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, train_dataset: Dataset, model_path: str):\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        total_correct = 0\n",
    "        total_tokens = 0\n",
    "        epoch_loss = 0\n",
    "\n",
    "        model.train()\n",
    "        for train_X, train_Y in tqdm(train_data_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "            train_X = train_X.to(DEVICE)\n",
    "            train_Y = train_Y.to(DEVICE)\n",
    "\n",
    "            output = model(train_X)\n",
    "\n",
    "            loss = criterion(output.view(-1, output.size(-1)),\n",
    "                             train_Y.view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            mask = (train_Y != PAD)\n",
    "            prediction = output.argmax(dim=-1)\n",
    "            total_correct += ((prediction == train_Y) & mask).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_acc = (total_correct / total_tokens) * 100\n",
    "        print(\n",
    "            f'Epochs: {epoch + 1} | Train Loss: {epoch_loss} \\\n",
    "            | Train Accuracy: {epoch_acc}\\n')\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f00cc728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: torch.nn.Module, val_dataset: torch.utils.data.Dataset):\n",
    "\n",
    "    val_data_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_correct_without_ending = 0\n",
    "    total_tokens_without_ending = 0\n",
    "    total_correct_ending = 0\n",
    "    total_tokens_ending = 0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for val_X, val_Y in tqdm(val_data_loader):\n",
    "            val_X = val_X.to(DEVICE)\n",
    "            val_Y = val_Y.to(DEVICE)\n",
    "\n",
    "            output = model(val_X)\n",
    "            prediction = output.argmax(dim=-1)\n",
    "\n",
    "            padding_mask = (val_Y == PAD)\n",
    "            shifted = torch.roll(val_X, shifts=-1, dims=1)\n",
    "            end_of_word_mask = (shifted == SPACE) | (shifted == PAD)\n",
    "\n",
    "            last_char_mask = end_of_word_mask & (~padding_mask)\n",
    "            rest_of_word_mask = (~end_of_word_mask) & (~padding_mask)\n",
    "            everything_mask = ~padding_mask\n",
    "\n",
    "            total_correct_ending += ((prediction == val_Y)\n",
    "                                     & last_char_mask).sum().item()\n",
    "            total_tokens_ending += last_char_mask.sum().item()\n",
    "\n",
    "            total_correct_without_ending += ((prediction == val_Y) &\n",
    "                                             rest_of_word_mask).sum().item()\n",
    "            total_tokens_without_ending += rest_of_word_mask.sum().item()\n",
    "\n",
    "            total_correct += ((prediction == val_Y) &\n",
    "                              everything_mask).sum().item()\n",
    "            total_tokens += everything_mask.sum().item()\n",
    "\n",
    "        val_accuracy = (total_correct / total_tokens) * 100\n",
    "        val_accuracy_without_ending = (total_correct_without_ending /\n",
    "                                       total_tokens_without_ending) * 100\n",
    "        val_accuracy_ending = (total_correct_ending /\n",
    "                               total_tokens_ending) * 100\n",
    "        print(\n",
    "            f\"Validation Accuracy (Overall): {val_accuracy:.2f}%\\n\" +\n",
    "            f\"Validation Accuracy (Without Last Character): {val_accuracy_without_ending:.2f}%\\n\" +\n",
    "            f\"Validation Accuracy (Last Character): {val_accuracy_ending:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ed91e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, encoded_sentence):\n",
    "    input_tensor = torch.tensor(\n",
    "        [encoded_sentence], dtype=torch.int64).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "    return outputs.argmax(dim=-1).squeeze(0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a98acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, model_path, input_path, output_path):\n",
    "\n",
    "    model_state_dict = torch.load(model_path, map_location=DEVICE)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        input_data = f.readlines()\n",
    "\n",
    "    output_list = []\n",
    "    output_csv = [[\"ID\", \"Label\"]]\n",
    "    current_id = 0\n",
    "\n",
    "    model.eval()\n",
    "    for sentence in input_data:\n",
    "        encoded_sentence = [CHAR2ID[char]\n",
    "                            for char in sentence if char in CHAR2ID]\n",
    "\n",
    "        predictions = predict(model, encoded_sentence)\n",
    "\n",
    "        diacritized_sentence = \"\"\n",
    "        for char_id, diacritic_id in zip(encoded_sentence, predictions):\n",
    "            char = ID2CHAR[char_id]\n",
    "            diacritic = ID2DIACRITIC[diacritic_id]\n",
    "            if char in ARABIC_LETTERS:\n",
    "                output_csv.append([current_id, diacritic_id])\n",
    "                current_id += 1\n",
    "            diacritized_sentence += char + diacritic\n",
    "\n",
    "        output_list.append(diacritized_sentence)\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for line in output_list:\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "    output_path_csv = os.path.splitext(output_path)[0] + \".csv\"\n",
    "    with open(output_path_csv, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03fc9994",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = generate_dataset(\"ArabicDataset\", \"../data/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eba76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = generate_dataset(\"ArabicDataset\", \"../data/val.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef3213c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = generate_model(\n",
    "    model_name=\"LSTMArabicModel\",\n",
    "    vocab_size=len(CHAR2ID),\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=len(DIACRITIC2ID),\n",
    "    PAD=PAD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2daacada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_state_dict = torch.load(model_path, map_location=DEVICE)\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e5422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_dataset, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df45c57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:14<00:00, 19.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy (Overall): 97.73%\n",
      "Validation Accuracy (Without Last Character): 98.26%\n",
      "Validation Accuracy (Last Character): 95.69%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7d24bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer(model, model_path, input_path, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
